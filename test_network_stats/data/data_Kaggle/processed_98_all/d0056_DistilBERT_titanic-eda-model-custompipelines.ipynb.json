{
    "nb_idx": 56,
    "nb_name": "d0056",
    "filename": "titanic-eda-model-custompipelines.ipynb",
    "filepath": "data/data_Kaggle/raw/titanic-eda-model-custompipelines.ipynb",
    "source": "# Importing general libraries:-\n\nimport numpy as np; \nfrom scipy.stats import mode;\nimport pandas as pd;\nfrom pandasql import sqldf;\nimport regex as re;\n\nimport matplotlib.pyplot as plt; \n%matplotlib inline\nimport seaborn as sns;\nsns.set_style('darkgrid');\n\nfrom warnings import filterwarnings;\nfrom termcolor import colored;\n\nfrom tqdm.notebook import tqdm;\n\nnp.random.seed(10); \n # Importing model specific libraries:-\nfrom sklearn_pandas import DataFrameMapper, gen_features;\n\nfrom sklearn.compose import make_column_selector;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.pipeline import make_pipeline, Pipeline ;\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, RobustScaler, OrdinalEncoder;\nfrom sklearn.impute import SimpleImputer;\n\nfrom sklearn.model_selection import KFold, GridSearchCV;\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier;\nfrom xgboost import XGBClassifier;\nfrom lightgbm import LGBMClassifier;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.linear_model import LogisticRegression;\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix; \n # Titanic- Machine Learning from disaster \n # Loading relevant data-sets:-\nxytrain = pd.read_csv('../input/titanic/train.csv', encoding = 'utf8');\nxtest = pd.read_csv('../input/titanic/test.csv', encoding = 'utf8');\n\n# Splitting the training data into features and target:-\nxtrain, ytrain = xytrain.drop('Survived', axis= 1), xytrain[['Survived']];\n\nprint(colored(F\"Train-Test dataframe lengths = {len(xytrain), len(xtest)}\", color= 'blue', attrs= ['bold']));\nprint(colored(F\"\\nTrain-set information\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.info());\nprint(colored(F\"\\nTrain-set description\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.describe().style.format('{:.2f}')); \n # 1. Data processing and visualization\n\nWe explore the data set, look into the features, study their distributions, assess nulls and understand the data effectively in this section.\nThese ideas will be used in the next section in the data pipeline. \n ### a. Target column details:- \n plt.subplots(1,1,figsize= (6,6));\nax = ytrain.value_counts().plot.bar(color= 'tab:blue');\nax.set_title(\"Surviver analysis for train set\", color = 'tab:blue', fontsize= 12);\nax.set_xlabel('Survival status', color= 'tab:blue');\nax.set_yticks(range(0, len(xtrain), 50));\nax.set_ylabel('Passengers', color= 'tab:blue');\nplt.xticks(rotation = 0);\nplt.show(); \n ### b. Passenger class and gender:- \n _ = xytrain.groupby(['Sex', 'Pclass']).agg(Survivors = pd.NamedAgg('Survived', np.sum),Passengers = pd.NamedAgg('Survived', np.size)).sort_index(level=[1,0])\n_['Survival_Rate'] = _['Survivors'] / _['Passengers'];\nprint(colored(f'\\nSurvival rate by gender and pclass\\n', color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format({'Survival_Rate':'{:.2%}'}))\n\n_0 = _.groupby(level= 0).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_0['Survival_Rate'] = _0['Survivors'] / _0['Passengers'];\n\n_1 = _.groupby(level= 1).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_1['Survival_Rate'] = _1['Survivors'] / _1['Passengers'];\n\nprint('\\n');\nfig, ax = plt.subplots(nrows= 1,ncols=2,figsize = (12,6), sharey= True);\nsns.barplot(x = _0.index, y = _0.Survival_Rate, palette = 'Blues', ax = ax[0]);\nsns.barplot(x = _1.index, y = _1.Survival_Rate, palette = 'Blues', ax= ax[1]);\nax[0].set_title(\"Survival analysis by gender\", color = 'tab:blue', fontsize = 12);\nax[1].set_title(\"Survival analysis by passenger class\", color = 'tab:blue', fontsize = 12);\nplt.yticks(np.arange(0,1,0.05),fontsize= 8, color = 'blue');\nplt.show()\n\ndel _, _0, _1; \n ### c. Age:- \n fig, ax = plt.subplots(1,2,figsize = (18,6));\nsns.histplot(x = xytrain['Age'], kde= True, palette = 'Blues', ax = ax[0]);\nax[0].set_title(f\"Overall age distribution analysis\", color = 'tab:blue', fontsize= 12 )\n\nsns.boxplot(x = xytrain.Pclass, y = xytrain.Age, palette = 'Blues', ax = ax[1]);\nax[1].set_title(f\"Age distribution per Pclass\", color = 'tab:blue', fontsize= 12);\n\nplt.show(); \n ### d. Ticket fare:- \n print(colored(f\"\\nTicket fare by Pclass and survivorship\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.groupby(['Pclass', 'Survived']).agg({'Fare': [np.amin, np.median, np.mean, np.amax]}).style.format('{:.2f}')) \n ### e. Null valued columns:- \n # Plotting null columns across the data-sets:-\ndef Plot_NullCol(df, df_type):\n    \"\"\"\n    This function plots the relevant data-set and scans for nulls across columns\n    Inputs- \n    df (dataframe):- The relevant data-frame for analysis\n    df_type (string):- Type of data (training/ test)\n    \"\"\";\n    \n    global xtrain;\n    _ = df.isna().sum(axis= 0);\n    print('\\n');\n    \n    plt.subplots(1,1, figsize= (8,8))\n    ax= _.plot.bar(color= 'tab:blue');\n    ax.set_title(f\"Columns with null values in {df_type} data\", color = 'tab:blue', fontsize= 12);\n    ax.set_yticks(range(0, len(xtrain),50));\n    ax.axhline(y= len(xtrain)/4, linewidth = 1.5, color= 'red');\n    ax.set_ylabel('Null values', color = 'tab:blue');\n    ax.set_xlabel('Features', color = 'tab:blue');\n    plt.show();\n    \n    print(colored(f\"Nulls in {df_type} data\\n\", color = 'blue'));\n    display(_);\n    del _;\n    \n\n# Plotting the train-test for nulls:-\nPlot_NullCol(xtrain, df_type= 'train');\nPlot_NullCol(xtest, df_type= 'test'); \n ### f. Cabin null inference in training set:- \n print(colored(f\"\\nCabin column null inferences in training data-set\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.assign(Cabin_cat = xtrain.Cabin.str[0:1]).groupby(['Cabin_cat', 'Pclass'], dropna= False).\\\n        agg({'Fare':[np.median, np.mean, np.amin, np.amax], \n             'PassengerId':[np.size], \n             'Survived': [np.sum]})\\\n        .style.format('{:,.0f}')); \n ## Creating pipeline adjutant functions and classes:-\n \n def TreatCabinNulls(df1:pd.DataFrame):\n    \"\"\"\n    This is an added function to treat the null valued cabin column in both the train and test data. \n    This is designed to impute the nulls instead of dropping the column entirely.\n    The treatment of nulls follows the below process-\n    1. Create a composite variable with the cabin category (1st letter in the cabin column) and the Pclass. \n       This is an interaction variable\n    2. Consider passengers with the same Pclass as the subject\n    3. Map the composite Cabin category with the null instances in the cabin based on the lowest fare difference. \n       Windowing SQL functions are used for the same\n    4. For cases where fare is not available, use the mode of the cabin category per Pclass. \n    \n    Input- df1 (dataframe):- Input dataframe without treatment\n    Returns- df (dataframe):- Dataframe with the cabin nulls treated \n    \"\"\";\n    \n    global xtrain, xtest;\n    \n    cabin_trmt_prf = \\\n    sqldf(f\"\"\" \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtrain \n    union all \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtest\n    \"\"\");\n\n    #  Creating the proxy variable with the fare and Pclass:-   \n    cabin_trmt_mst = \\\n    sqldf(f\"\"\"\n    select A1.PID1 as PassengerId, A1.Pclass, A1.Cabin_Ctg_Lbl\n    from \n    (\n    select a.PassengerId as PID1,a.Pclass, b.PassengerId as PID2, b.Cabin_Ctg_Lbl, a.Fare as Fare1, b.Fare as Fare2, abs(a.Fare - b.Fare) as Fare_Diff,\n    row_number() over(partition by a.PassengerId order by abs(a.Fare - b.Fare) asc) as Fare_Diff_Rank\n    from \n    (select PassengerId, Pclass, Fare, Embarked from cabin_trmt_prf WHERE Cabin is null) A \n    inner join \n    (\n    select PassengerId, Pclass, Fare, (cast(Pclass as varchar(1)) || substr(Cabin,1,1)) AS Cabin_Ctg_Lbl, Embarked \n    from cabin_trmt_prf \n    where Cabin is not null\n    ) B on (A.Pclass = B.Pclass and abs(a.Fare - b.Fare) <= 50)\n    ) A1\n    where A1.Fare_Diff_Rank == 1\n    \"\"\");\n    \n    #  Finally appending the nulls still present with the mode of the Pclass and Category label:-\n    cabin_md_sum = \\\n    sqldf(\"\"\"\n    select a.* from \n    (\n    select Pclass, Cabin_Ctg_Lbl, count(PassengerId) as cnt, row_number() over (order by count(PassengerId) desc) as rank_id\n    from cabin_trmt_mst \n    group by Pclass, Cabin_Ctg_Lbl\n    ) a\n    where a.rank_id = 1\n    \"\"\");\n \n    # Mapping the interaction variable to the relevant table:-    \n    df = df1.copy();   \n    df = sqldf(\"\"\"\n    select a.*, coalesce(coalesce(b.Cabin_Ctg_Lbl, cast(a.Pclass as varchar(1)) || substr(a.Cabin,1,1)), c.Cabin_Ctg_Lbl) as Cabin_Class_Lbl \n    from df a \n    left join cabin_trmt_mst b on a.PassengerId = B.PassengerId\n    left join cabin_md_sum c on a.Pclass = c.Pclass\n    \"\"\");  \n    \n    del cabin_trmt_mst, cabin_trmt_prf, cabin_md_sum; \n    return df;   \n def Xform_Data(df1:pd.DataFrame): \n    \"\"\"\n    This function does the below tasks:-\n    1. Creates a title attribute from the name column\n    2. Assigns 'others' to uncommon titles\n    3. Creates a flag for 'child' from the title. This is used to fill in the age nulls.\n    4. Creates a 'family members count' column from the Sibsp and Parch attributes\n    5. Drops superfluous columns\n    \n    Input- df (dataframe):- Analysis dataframe\n    Returns- df (dataframe):- Modified dataframe   \n    \"\"\";\n    \n    filterwarnings(action= \"ignore\");\n    df = df1.copy();\n    \n    df['Title'] = df['Name'].apply(lambda x: re.findall(r\"\\S+\\. \",x)[0].strip()[0:-1]);\n    df['Title'].loc[~df.Title.isin(['Mr', 'Mrs', 'Miss', 'Master'])] = 'Others';\n    df['Is_child'] = np.select([df['Title'].str.lower() == 'master'], ['Y'], 'N');    \n    df['Nb_Fmly_Mem'] = df['SibSp'].fillna(0) + df['Parch'].fillna(0);  \n    df = df.drop(['PassengerId', 'Ticket', 'Name', 'Cabin'], axis= 1);  \n    \n    return df; \n class AgeImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the age null values with the child/ adult gender based medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the child and adult groups for filling nulls\";\n        self.median_ = X.groupby(['Is_child','Sex'])[['Age']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Is_child', 'Sex'], suffixes= ('', '_Median'));\n        X1['Age'] = X1['Age'].fillna(X1.Age_Median);\n        X1 = X1.drop(['Age_Median'], axis= 1);\n        return X1; \n class FareImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the ticket fare null values with the Pclass medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the Pclass groups for filling nulls\";\n        self.median_ = X.groupby(['Pclass'])[['Fare']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Pclass'], suffixes= ('', '_Median'));\n        X1['Fare'] = X1['Fare'].fillna(X1.Fare_Median);\n        X1 = X1.drop(['Fare_Median'], axis= 1);\n        return X1; \n class AgeFareBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class develops bins for the age and ticket fare, to foster stability and offer better and stable predictability.\n    Training set parameters are developed with the fit function. \n    \"\"\";\n    \n    def __init__(self, nb_age_bins: int = 5, nb_fare_bins:int = 5): \n        self.nb_age_bins = nb_age_bins\n        self.nb_fare_bins = nb_fare_bins\n    \n    def fit(self, X, y= None, **fit_params):\n        \"\"\"This function calculates the bins for the age and fare columns respectively\"\"\";     \n        self.age_bins_ = pd.qcut(X['Age'], q = self.nb_age_bins, retbins= True, labels = range(1,self.nb_age_bins + 1,1))[1];\n        self.fare_bins_ = pd.qcut(X['Fare'], q = self.nb_fare_bins, retbins= True, labels = range(1,self.nb_fare_bins + 1,1))[1];   \n        return self;\n        \n    def transform(self, X, y=None, **transform_param):\n        \"\"\"This function applies the binned results to the relevant dataframe and returns the labelled column\"\"\";        \n        X1 = X.copy();\n        \n        self.age_bins_[0] = 0.0; self.fare_bins_[0] = 0.0;\n        self.age_bins_[-1] = np.inf; self.fare_bins_[-1] = np.inf;\n        \n        X1['Age_Bin'] = pd.cut(X1['Age'], self.age_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True);\n        X1['Fare_Bin'] = pd.cut(X1['Fare'], self.fare_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True); \n               \n        global df_col; df_col = list(X1.columns);\n        return X1;       \n # 2. Model pipeline development\n\nKey pipeline steps:-\n* Treat nulls in cabin- This step treats nulls in cabin using Pclass and Fare, also fills up remaining nulls after treatment with the Pclass mode\n* Transform data- This step adds the new features and drops some redundant features.\n* Impute Embarked- This step imputes the null values in the column with the mode\n* Label encoder- This is used for the selected object columns only\n* Impute age- This is used to impute age based on the grouped median of child/ adult and gender\n* Impute fare- This fills fare column nulls based on the median of Pclass fare\n* Bin Age Fare- This bins the age and fare columns (this is not used in the latest version)\n* Robust Scaler- This is used for numerical columns only as selected in the column list \n nb_age_bins = 5;\nnb_fare_bins = 5;\nenc_col_lst = ['Sex', 'Embarked', 'Title', 'Is_child'];\nstd_col_lst = ['Age', 'Fare', 'SibSp', 'Parch', 'Nb_Fmly_Mem' ]\n\nData_Processor = \\\nPipeline(verbose=True, \n         steps= \\\n         [('Imp_Cabin', FunctionTransformer(func= TreatCabinNulls)),\n          ('Xform_Data', FunctionTransformer(func= Xform_Data)),\n          ('Imp_Embk', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                             features= [(['Embarked'], SimpleImputer(strategy= 'most_frequent'))])\n          ),\n          ('Lbl_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                            features = gen_features([col.split(' ') for col in enc_col_lst], [LabelEncoder]))\n          ),\n          ('Imp_Age', AgeImputer()),\n          ('Imp_Fare',FareImputer()),\n          ('Ord_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                      features= [(['Cabin_Class_Lbl'], OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= 49))])\n          ),\n#           ('BinAgeFare', AgeFareBinner(nb_age_bins = nb_age_bins, nb_fare_bins = nb_fare_bins)),\n          ('Std', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                  features = gen_features([col.split(' ') for col in std_col_lst], [RobustScaler])))\n         ]\n        ); \n print(colored(f\"\\nPipeline details\\n\", color = 'blue'));\nData_Processor.fit(xtrain, ytrain);\n\n# Train-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the training set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtrain = Data_Processor.transform(xtrain);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtrain.isna().sum(axis=0));\nprint(colored(f'\\nData description after pipeline\\n', color = 'blue'));\ndisplay(Xtrain.describe().style.format('{:.2f}'));\n\n# Test-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the test set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtest = Data_Processor.transform(xtest);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtest.isna().sum(axis=0)); \n ### Model parameter setting and implementation:-\n\nKey models and description:-\n1. Logistic Regression\n2. SVC classifier\n3. Single tree\n4. Ensemble random forest\n5. Gradient boosting machine\n6. Light GBM\n7. XgBoost Classifier\n\nModel outputs are stored in 2 tables- \n1. Model parameters profile (mdl_param_prf)- This stores the model name and relevant score metrics\n2. Model prediction profile (mdl_pred_prf) - This stores the test set predictions from each model (best estimator) \n # Creating a master dictionary for the model parameters and class instances:-\nmdl_mst_dict = \\\n{\n'Logistic': [LogisticRegression(random_state = 10), {}],\n'SVC': [SVC(random_state = 10), {'C': range(3,10,1)}],\n'DTree' : [DecisionTreeClassifier(random_state= 10), {'max_depth': range(4,8,1), 'min_samples_leaf' : range(3,12,1)}],\n'RandomForest': [RandomForestClassifier(random_state =10), \n                 {'n_estimators': range(50,300,25), 'max_depth': range(4,7,1)}],\n'GBM': [GradientBoostingClassifier(random_state= 10), {'max_depth' : range(2,6,1)}],\n'LGBM': [LGBMClassifier(random_state = 10),{}],\n'XgBoost': [XGBClassifier(eval_metric= 'logloss'), {}]\n};\n\ncv = None;\n\n# Creating model output storage objects:-\nmdl_param_prf = pd.DataFrame(data= None, index= list(mdl_mst_dict.keys()), dtype= np.float32,\n                             columns= ['Precision_Score', 'Recall_Score', 'F1_Score', 'ROC_AUC_Score', 'Accuracy_Score']);\nmdl_pred_prf = pd.DataFrame(data= None, index= None, columns= None, dtype= np.int32); \n Implementation routine- \n1. Load the relevant model\n2. Fit the model with the grid search parameters\n3. Create relevant scores for the training set\n4. Extract test set predictions\n5. Integrate the data accordingly into the output tables \n # Implementing all models:-\nfor mdl_lbl, mdl_param in tqdm(mdl_mst_dict.items()):\n    print(colored(f\"\\nCurrent model is {mdl_lbl}\", color = 'red', attrs= ['bold', 'dark']));\n    grid = GridSearchCV(estimator = mdl_param[0], param_grid = mdl_param[1], scoring='accuracy', refit=True,cv=cv);\n    grid.fit(Xtrain, ytrain);\n    print(colored(f\"\"\"Best estimator is \\n{grid.best_estimator_}\\n\"\"\", color = 'blue'));\n    \n    ytrain_pred = grid.predict(Xtrain);  \n    print(colored(f\"Confusion matrix\\n{confusion_matrix(ytrain['Survived'].values, ytrain_pred)}\", color = 'blue'));\n    \n    mdl_param_prf.loc[mdl_lbl] = (precision_score(ytrain['Survived'].values, ytrain_pred),\n                                  recall_score(ytrain['Survived'].values, ytrain_pred),\n                                  f1_score(ytrain['Survived'].values, ytrain_pred),\n                                  roc_auc_score(ytrain['Survived'].values, ytrain_pred),\n                                  accuracy_score(ytrain['Survived'].values, ytrain_pred)\n                                 );\n    \n    mdl_pred_prf = pd.concat((mdl_pred_prf,pd.DataFrame(grid.predict(Xtest), index = Xtest.index, columns = [mdl_lbl], dtype= np.int32)),\n                             axis= 1, join='outer');\n\nprint(colored(f\"\\n\\nTraining set scores across models:-\\n\", color = 'blue', attrs= ['bold', 'dark']))\ndisplay(mdl_param_prf.style.format('{:.2%}')); \n # 3. Submission file preparation\n\nWe explore a few options from the developed models and prepare the submissio file in this section. This is the last section in the model framework. \n print(colored(f\"Sample submission file\\n\", color= 'blue', attrs= ['dark', 'bold']));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').head(5));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').tail(5)); \n # Adding new columns for specific mode values:-\nmdl_pred_prf['AllModels'] = mode(mdl_pred_prf, axis=1)[0];\nmdl_pred_prf['BoostedTree'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost']], axis=1)[0];\nmdl_pred_prf['Ensemble'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost', 'RandomForest']], axis=1)[0]; \n sel_col_nm = 'AllModels';\npd.DataFrame(mdl_pred_prf[sel_col_nm].values, index = xtest.PassengerId, \n             columns = ['Survived'], dtype= np.int32).reset_index().to_csv(\"Submission.csv\", index= False);",
    "code_source": "# Importing general libraries:-\n\nimport numpy as np; \nfrom scipy.stats import mode;\nimport pandas as pd;\nfrom pandasql import sqldf;\nimport regex as re;\n\nimport matplotlib.pyplot as plt; \n%matplotlib inline\nimport seaborn as sns;\nsns.set_style('darkgrid');\n\nfrom warnings import filterwarnings;\nfrom termcolor import colored;\n\nfrom tqdm.notebook import tqdm;\n\nnp.random.seed(10); \n # Importing model specific libraries:-\nfrom sklearn_pandas import DataFrameMapper, gen_features;\n\nfrom sklearn.compose import make_column_selector;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.pipeline import make_pipeline, Pipeline ;\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, RobustScaler, OrdinalEncoder;\nfrom sklearn.impute import SimpleImputer;\n\nfrom sklearn.model_selection import KFold, GridSearchCV;\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier;\nfrom xgboost import XGBClassifier;\nfrom lightgbm import LGBMClassifier;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.linear_model import LogisticRegression;\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix; \n # Loading relevant data-sets:-\nxytrain = pd.read_csv('../input/titanic/train.csv', encoding = 'utf8');\nxtest = pd.read_csv('../input/titanic/test.csv', encoding = 'utf8');\n\n# Splitting the training data into features and target:-\nxtrain, ytrain = xytrain.drop('Survived', axis= 1), xytrain[['Survived']];\n\nprint(colored(F\"Train-Test dataframe lengths = {len(xytrain), len(xtest)}\", color= 'blue', attrs= ['bold']));\nprint(colored(F\"\\nTrain-set information\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.info());\nprint(colored(F\"\\nTrain-set description\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.describe().style.format('{:.2f}')); \n plt.subplots(1,1,figsize= (6,6));\nax = ytrain.value_counts().plot.bar(color= 'tab:blue');\nax.set_title(\"Surviver analysis for train set\", color = 'tab:blue', fontsize= 12);\nax.set_xlabel('Survival status', color= 'tab:blue');\nax.set_yticks(range(0, len(xtrain), 50));\nax.set_ylabel('Passengers', color= 'tab:blue');\nplt.xticks(rotation = 0);\nplt.show(); \n _ = xytrain.groupby(['Sex', 'Pclass']).agg(Survivors = pd.NamedAgg('Survived', np.sum),Passengers = pd.NamedAgg('Survived', np.size)).sort_index(level=[1,0])\n_['Survival_Rate'] = _['Survivors'] / _['Passengers'];\nprint(colored(f'\\nSurvival rate by gender and pclass\\n', color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format({'Survival_Rate':'{:.2%}'}))\n\n_0 = _.groupby(level= 0).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_0['Survival_Rate'] = _0['Survivors'] / _0['Passengers'];\n\n_1 = _.groupby(level= 1).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_1['Survival_Rate'] = _1['Survivors'] / _1['Passengers'];\n\nprint('\\n');\nfig, ax = plt.subplots(nrows= 1,ncols=2,figsize = (12,6), sharey= True);\nsns.barplot(x = _0.index, y = _0.Survival_Rate, palette = 'Blues', ax = ax[0]);\nsns.barplot(x = _1.index, y = _1.Survival_Rate, palette = 'Blues', ax= ax[1]);\nax[0].set_title(\"Survival analysis by gender\", color = 'tab:blue', fontsize = 12);\nax[1].set_title(\"Survival analysis by passenger class\", color = 'tab:blue', fontsize = 12);\nplt.yticks(np.arange(0,1,0.05),fontsize= 8, color = 'blue');\nplt.show()\n\ndel _, _0, _1; \n fig, ax = plt.subplots(1,2,figsize = (18,6));\nsns.histplot(x = xytrain['Age'], kde= True, palette = 'Blues', ax = ax[0]);\nax[0].set_title(f\"Overall age distribution analysis\", color = 'tab:blue', fontsize= 12 )\n\nsns.boxplot(x = xytrain.Pclass, y = xytrain.Age, palette = 'Blues', ax = ax[1]);\nax[1].set_title(f\"Age distribution per Pclass\", color = 'tab:blue', fontsize= 12);\n\nplt.show(); \n print(colored(f\"\\nTicket fare by Pclass and survivorship\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.groupby(['Pclass', 'Survived']).agg({'Fare': [np.amin, np.median, np.mean, np.amax]}).style.format('{:.2f}')) \n # Plotting null columns across the data-sets:-\ndef Plot_NullCol(df, df_type):\n    \"\"\"\n    This function plots the relevant data-set and scans for nulls across columns\n    Inputs- \n    df (dataframe):- The relevant data-frame for analysis\n    df_type (string):- Type of data (training/ test)\n    \"\"\";\n    \n    global xtrain;\n    _ = df.isna().sum(axis= 0);\n    print('\\n');\n    \n    plt.subplots(1,1, figsize= (8,8))\n    ax= _.plot.bar(color= 'tab:blue');\n    ax.set_title(f\"Columns with null values in {df_type} data\", color = 'tab:blue', fontsize= 12);\n    ax.set_yticks(range(0, len(xtrain),50));\n    ax.axhline(y= len(xtrain)/4, linewidth = 1.5, color= 'red');\n    ax.set_ylabel('Null values', color = 'tab:blue');\n    ax.set_xlabel('Features', color = 'tab:blue');\n    plt.show();\n    \n    print(colored(f\"Nulls in {df_type} data\\n\", color = 'blue'));\n    display(_);\n    del _;\n    \n\n# Plotting the train-test for nulls:-\nPlot_NullCol(xtrain, df_type= 'train');\nPlot_NullCol(xtest, df_type= 'test'); \n print(colored(f\"\\nCabin column null inferences in training data-set\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.assign(Cabin_cat = xtrain.Cabin.str[0:1]).groupby(['Cabin_cat', 'Pclass'], dropna= False).\\\n        agg({'Fare':[np.median, np.mean, np.amin, np.amax], \n             'PassengerId':[np.size], \n             'Survived': [np.sum]})\\\n        .style.format('{:,.0f}')); \n def TreatCabinNulls(df1:pd.DataFrame):\n    \"\"\"\n    This is an added function to treat the null valued cabin column in both the train and test data. \n    This is designed to impute the nulls instead of dropping the column entirely.\n    The treatment of nulls follows the below process-\n    1. Create a composite variable with the cabin category (1st letter in the cabin column) and the Pclass. \n       This is an interaction variable\n    2. Consider passengers with the same Pclass as the subject\n    3. Map the composite Cabin category with the null instances in the cabin based on the lowest fare difference. \n       Windowing SQL functions are used for the same\n    4. For cases where fare is not available, use the mode of the cabin category per Pclass. \n    \n    Input- df1 (dataframe):- Input dataframe without treatment\n    Returns- df (dataframe):- Dataframe with the cabin nulls treated \n    \"\"\";\n    \n    global xtrain, xtest;\n    \n    cabin_trmt_prf = \\\n    sqldf(f\"\"\" \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtrain \n    union all \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtest\n    \"\"\");\n\n    #  Creating the proxy variable with the fare and Pclass:-   \n    cabin_trmt_mst = \\\n    sqldf(f\"\"\"\n    select A1.PID1 as PassengerId, A1.Pclass, A1.Cabin_Ctg_Lbl\n    from \n    (\n    select a.PassengerId as PID1,a.Pclass, b.PassengerId as PID2, b.Cabin_Ctg_Lbl, a.Fare as Fare1, b.Fare as Fare2, abs(a.Fare - b.Fare) as Fare_Diff,\n    row_number() over(partition by a.PassengerId order by abs(a.Fare - b.Fare) asc) as Fare_Diff_Rank\n    from \n    (select PassengerId, Pclass, Fare, Embarked from cabin_trmt_prf WHERE Cabin is null) A \n    inner join \n    (\n    select PassengerId, Pclass, Fare, (cast(Pclass as varchar(1)) || substr(Cabin,1,1)) AS Cabin_Ctg_Lbl, Embarked \n    from cabin_trmt_prf \n    where Cabin is not null\n    ) B on (A.Pclass = B.Pclass and abs(a.Fare - b.Fare) <= 50)\n    ) A1\n    where A1.Fare_Diff_Rank == 1\n    \"\"\");\n    \n    #  Finally appending the nulls still present with the mode of the Pclass and Category label:-\n    cabin_md_sum = \\\n    sqldf(\"\"\"\n    select a.* from \n    (\n    select Pclass, Cabin_Ctg_Lbl, count(PassengerId) as cnt, row_number() over (order by count(PassengerId) desc) as rank_id\n    from cabin_trmt_mst \n    group by Pclass, Cabin_Ctg_Lbl\n    ) a\n    where a.rank_id = 1\n    \"\"\");\n \n    # Mapping the interaction variable to the relevant table:-    \n    df = df1.copy();   \n    df = sqldf(\"\"\"\n    select a.*, coalesce(coalesce(b.Cabin_Ctg_Lbl, cast(a.Pclass as varchar(1)) || substr(a.Cabin,1,1)), c.Cabin_Ctg_Lbl) as Cabin_Class_Lbl \n    from df a \n    left join cabin_trmt_mst b on a.PassengerId = B.PassengerId\n    left join cabin_md_sum c on a.Pclass = c.Pclass\n    \"\"\");  \n    \n    del cabin_trmt_mst, cabin_trmt_prf, cabin_md_sum; \n    return df;   \n def Xform_Data(df1:pd.DataFrame): \n    \"\"\"\n    This function does the below tasks:-\n    1. Creates a title attribute from the name column\n    2. Assigns 'others' to uncommon titles\n    3. Creates a flag for 'child' from the title. This is used to fill in the age nulls.\n    4. Creates a 'family members count' column from the Sibsp and Parch attributes\n    5. Drops superfluous columns\n    \n    Input- df (dataframe):- Analysis dataframe\n    Returns- df (dataframe):- Modified dataframe   \n    \"\"\";\n    \n    filterwarnings(action= \"ignore\");\n    df = df1.copy();\n    \n    df['Title'] = df['Name'].apply(lambda x: re.findall(r\"\\S+\\. \",x)[0].strip()[0:-1]);\n    df['Title'].loc[~df.Title.isin(['Mr', 'Mrs', 'Miss', 'Master'])] = 'Others';\n    df['Is_child'] = np.select([df['Title'].str.lower() == 'master'], ['Y'], 'N');    \n    df['Nb_Fmly_Mem'] = df['SibSp'].fillna(0) + df['Parch'].fillna(0);  \n    df = df.drop(['PassengerId', 'Ticket', 'Name', 'Cabin'], axis= 1);  \n    \n    return df; \n class AgeImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the age null values with the child/ adult gender based medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the child and adult groups for filling nulls\";\n        self.median_ = X.groupby(['Is_child','Sex'])[['Age']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Is_child', 'Sex'], suffixes= ('', '_Median'));\n        X1['Age'] = X1['Age'].fillna(X1.Age_Median);\n        X1 = X1.drop(['Age_Median'], axis= 1);\n        return X1; \n class FareImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the ticket fare null values with the Pclass medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the Pclass groups for filling nulls\";\n        self.median_ = X.groupby(['Pclass'])[['Fare']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Pclass'], suffixes= ('', '_Median'));\n        X1['Fare'] = X1['Fare'].fillna(X1.Fare_Median);\n        X1 = X1.drop(['Fare_Median'], axis= 1);\n        return X1; \n class AgeFareBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class develops bins for the age and ticket fare, to foster stability and offer better and stable predictability.\n    Training set parameters are developed with the fit function. \n    \"\"\";\n    \n    def __init__(self, nb_age_bins: int = 5, nb_fare_bins:int = 5): \n        self.nb_age_bins = nb_age_bins\n        self.nb_fare_bins = nb_fare_bins\n    \n    def fit(self, X, y= None, **fit_params):\n        \"\"\"This function calculates the bins for the age and fare columns respectively\"\"\";     \n        self.age_bins_ = pd.qcut(X['Age'], q = self.nb_age_bins, retbins= True, labels = range(1,self.nb_age_bins + 1,1))[1];\n        self.fare_bins_ = pd.qcut(X['Fare'], q = self.nb_fare_bins, retbins= True, labels = range(1,self.nb_fare_bins + 1,1))[1];   \n        return self;\n        \n    def transform(self, X, y=None, **transform_param):\n        \"\"\"This function applies the binned results to the relevant dataframe and returns the labelled column\"\"\";        \n        X1 = X.copy();\n        \n        self.age_bins_[0] = 0.0; self.fare_bins_[0] = 0.0;\n        self.age_bins_[-1] = np.inf; self.fare_bins_[-1] = np.inf;\n        \n        X1['Age_Bin'] = pd.cut(X1['Age'], self.age_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True);\n        X1['Fare_Bin'] = pd.cut(X1['Fare'], self.fare_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True); \n               \n        global df_col; df_col = list(X1.columns);\n        return X1;       \n nb_age_bins = 5;\nnb_fare_bins = 5;\nenc_col_lst = ['Sex', 'Embarked', 'Title', 'Is_child'];\nstd_col_lst = ['Age', 'Fare', 'SibSp', 'Parch', 'Nb_Fmly_Mem' ]\n\nData_Processor = \\\nPipeline(verbose=True, \n         steps= \\\n         [('Imp_Cabin', FunctionTransformer(func= TreatCabinNulls)),\n          ('Xform_Data', FunctionTransformer(func= Xform_Data)),\n          ('Imp_Embk', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                             features= [(['Embarked'], SimpleImputer(strategy= 'most_frequent'))])\n          ),\n          ('Lbl_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                            features = gen_features([col.split(' ') for col in enc_col_lst], [LabelEncoder]))\n          ),\n          ('Imp_Age', AgeImputer()),\n          ('Imp_Fare',FareImputer()),\n          ('Ord_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                      features= [(['Cabin_Class_Lbl'], OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= 49))])\n          ),\n#           ('BinAgeFare', AgeFareBinner(nb_age_bins = nb_age_bins, nb_fare_bins = nb_fare_bins)),\n          ('Std', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                  features = gen_features([col.split(' ') for col in std_col_lst], [RobustScaler])))\n         ]\n        ); \n print(colored(f\"\\nPipeline details\\n\", color = 'blue'));\nData_Processor.fit(xtrain, ytrain);\n\n# Train-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the training set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtrain = Data_Processor.transform(xtrain);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtrain.isna().sum(axis=0));\nprint(colored(f'\\nData description after pipeline\\n', color = 'blue'));\ndisplay(Xtrain.describe().style.format('{:.2f}'));\n\n# Test-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the test set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtest = Data_Processor.transform(xtest);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtest.isna().sum(axis=0)); \n # Creating a master dictionary for the model parameters and class instances:-\nmdl_mst_dict = \\\n{\n'Logistic': [LogisticRegression(random_state = 10), {}],\n'SVC': [SVC(random_state = 10), {'C': range(3,10,1)}],\n'DTree' : [DecisionTreeClassifier(random_state= 10), {'max_depth': range(4,8,1), 'min_samples_leaf' : range(3,12,1)}],\n'RandomForest': [RandomForestClassifier(random_state =10), \n                 {'n_estimators': range(50,300,25), 'max_depth': range(4,7,1)}],\n'GBM': [GradientBoostingClassifier(random_state= 10), {'max_depth' : range(2,6,1)}],\n'LGBM': [LGBMClassifier(random_state = 10),{}],\n'XgBoost': [XGBClassifier(eval_metric= 'logloss'), {}]\n};\n\ncv = None;\n\n# Creating model output storage objects:-\nmdl_param_prf = pd.DataFrame(data= None, index= list(mdl_mst_dict.keys()), dtype= np.float32,\n                             columns= ['Precision_Score', 'Recall_Score', 'F1_Score', 'ROC_AUC_Score', 'Accuracy_Score']);\nmdl_pred_prf = pd.DataFrame(data= None, index= None, columns= None, dtype= np.int32); \n # Implementing all models:-\nfor mdl_lbl, mdl_param in tqdm(mdl_mst_dict.items()):\n    print(colored(f\"\\nCurrent model is {mdl_lbl}\", color = 'red', attrs= ['bold', 'dark']));\n    grid = GridSearchCV(estimator = mdl_param[0], param_grid = mdl_param[1], scoring='accuracy', refit=True,cv=cv);\n    grid.fit(Xtrain, ytrain);\n    print(colored(f\"\"\"Best estimator is \\n{grid.best_estimator_}\\n\"\"\", color = 'blue'));\n    \n    ytrain_pred = grid.predict(Xtrain);  \n    print(colored(f\"Confusion matrix\\n{confusion_matrix(ytrain['Survived'].values, ytrain_pred)}\", color = 'blue'));\n    \n    mdl_param_prf.loc[mdl_lbl] = (precision_score(ytrain['Survived'].values, ytrain_pred),\n                                  recall_score(ytrain['Survived'].values, ytrain_pred),\n                                  f1_score(ytrain['Survived'].values, ytrain_pred),\n                                  roc_auc_score(ytrain['Survived'].values, ytrain_pred),\n                                  accuracy_score(ytrain['Survived'].values, ytrain_pred)\n                                 );\n    \n    mdl_pred_prf = pd.concat((mdl_pred_prf,pd.DataFrame(grid.predict(Xtest), index = Xtest.index, columns = [mdl_lbl], dtype= np.int32)),\n                             axis= 1, join='outer');\n\nprint(colored(f\"\\n\\nTraining set scores across models:-\\n\", color = 'blue', attrs= ['bold', 'dark']))\ndisplay(mdl_param_prf.style.format('{:.2%}')); \n print(colored(f\"Sample submission file\\n\", color= 'blue', attrs= ['dark', 'bold']));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').head(5));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').tail(5)); \n # Adding new columns for specific mode values:-\nmdl_pred_prf['AllModels'] = mode(mdl_pred_prf, axis=1)[0];\nmdl_pred_prf['BoostedTree'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost']], axis=1)[0];\nmdl_pred_prf['Ensemble'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost', 'RandomForest']], axis=1)[0]; \n sel_col_nm = 'AllModels';\npd.DataFrame(mdl_pred_prf[sel_col_nm].values, index = xtest.PassengerId, \n             columns = ['Survived'], dtype= np.int32).reset_index().to_csv(\"Submission.csv\", index= False);",
    "markdown_source": "# Titanic- Machine Learning from disaster \n # 1. Data processing and visualization\n\nWe explore the data set, look into the features, study their distributions, assess nulls and understand the data effectively in this section.\nThese ideas will be used in the next section in the data pipeline. \n ### a. Target column details:- \n ### b. Passenger class and gender:- \n ### c. Age:- \n ### d. Ticket fare:- \n ### e. Null valued columns:- \n ### f. Cabin null inference in training set:- \n ## Creating pipeline adjutant functions and classes:-\n \n # 2. Model pipeline development\n\nKey pipeline steps:-\n* Treat nulls in cabin- This step treats nulls in cabin using Pclass and Fare, also fills up remaining nulls after treatment with the Pclass mode\n* Transform data- This step adds the new features and drops some redundant features.\n* Impute Embarked- This step imputes the null values in the column with the mode\n* Label encoder- This is used for the selected object columns only\n* Impute age- This is used to impute age based on the grouped median of child/ adult and gender\n* Impute fare- This fills fare column nulls based on the median of Pclass fare\n* Bin Age Fare- This bins the age and fare columns (this is not used in the latest version)\n* Robust Scaler- This is used for numerical columns only as selected in the column list \n ### Model parameter setting and implementation:-\n\nKey models and description:-\n1. Logistic Regression\n2. SVC classifier\n3. Single tree\n4. Ensemble random forest\n5. Gradient boosting machine\n6. Light GBM\n7. XgBoost Classifier\n\nModel outputs are stored in 2 tables- \n1. Model parameters profile (mdl_param_prf)- This stores the model name and relevant score metrics\n2. Model prediction profile (mdl_pred_prf) - This stores the test set predictions from each model (best estimator) \n Implementation routine- \n1. Load the relevant model\n2. Fit the model with the grid search parameters\n3. Create relevant scores for the training set\n4. Extract test set predictions\n5. Integrate the data accordingly into the output tables \n # 3. Submission file preparation\n\nWe explore a few options from the developed models and prepare the submissio file in this section. This is the last section in the model framework.",
    "n_cells": 34,
    "n_code_cells": 21,
    "n_markdown_cells": 13,
    "n_raw_cells": 0,
    "n_outputs": 21,
    "r_code_cells": 0.6176470588235294,
    "r_markdown_cells": 0.38235294117647056,
    "r_raw_cells": 0.0,
    "r_outputs": 1.0,
    "n_exceptions": 0,
    "r_exceptions": 0.0,
    "n_lines": 411,
    "n_lines_code": 364,
    "n_lines_markdown": 47,
    "lines_per_cell": [
        19,
        18,
        1,
        12,
        4,
        1,
        8,
        1,
        21,
        1,
        8,
        1,
        2,
        1,
        30,
        1,
        6,
        2,
        68,
        23,
        13,
        13,
        28,
        11,
        26,
        18,
        14,
        19,
        6,
        22,
        3,
        3,
        4,
        3
    ],
    "lines_per_code_cell": [
        19,
        18,
        12,
        8,
        21,
        8,
        2,
        30,
        6,
        68,
        23,
        13,
        13,
        28,
        26,
        18,
        19,
        22,
        3,
        4,
        3
    ],
    "lines_per_markdown_cell": [
        1,
        4,
        1,
        1,
        1,
        1,
        1,
        1,
        2,
        11,
        14,
        6,
        3
    ],
    "ave_lines_per_cell": 12.088235294117647,
    "ave_lines_per_code_cell": 17.333333333333332,
    "ave_lines_per_markdown_cell": 3.6153846153846154,
    "max_lines_per_cell": 68,
    "max_lines_per_code_cell": 68,
    "max_lines_per_markdown_cell": 14,
    "min_lines_per_cell": 1,
    "min_lines_per_code_cell": 2,
    "min_lines_per_markdown_cell": 1,
    "n_chars": 20054,
    "n_chars_code": 17859,
    "n_chars_markdown": 2195,
    "chars_per_cell": [
        379,
        921,
        41,
        666,
        248,
        30,
        353,
        35,
        1137,
        12,
        388,
        20,
        229,
        28,
        1004,
        45,
        394,
        54,
        2848,
        989,
        818,
        787,
        1564,
        787,
        1419,
        777,
        478,
        1017,
        236,
        1401,
        181,
        264,
        297,
        207
    ],
    "chars_per_code_cell": [
        379,
        921,
        666,
        353,
        1137,
        388,
        229,
        1004,
        394,
        2848,
        989,
        818,
        787,
        1564,
        1419,
        777,
        1017,
        1401,
        264,
        297,
        207
    ],
    "chars_per_markdown_cell": [
        41,
        248,
        30,
        35,
        12,
        20,
        28,
        45,
        54,
        787,
        478,
        236,
        181
    ],
    "ave_chars_per_line": 48.79318734793188,
    "ave_chars_per_cell": 589.8235294117648,
    "ave_chars_per_code_cell": 850.4285714285714,
    "ave_chars_per_markdown_cell": 168.84615384615384,
    "max_chars_per_cell": 2848,
    "max_chars_per_code_cell": 2848,
    "max_chars_per_markdownell": 787,
    "min_chars_per_cell": 12,
    "min_chars_per_code_cell": 207,
    "min_chars_per_markdown_cell": 12,
    "r_lines_code": 0.8856447688564477,
    "r_lines_markdown": 0.11435523114355231,
    "r_chars_markdown": 0.10945447292310762,
    "r_chars_code": 0.8905455270768924,
    "all_cells": [
        {
            "source": "# Importing general libraries:-\n\nimport numpy as np; \nfrom scipy.stats import mode;\nimport pandas as pd;\nfrom pandasql import sqldf;\nimport regex as re;\n\nimport matplotlib.pyplot as plt; \n%matplotlib inline\nimport seaborn as sns;\nsns.set_style('darkgrid');\n\nfrom warnings import filterwarnings;\nfrom termcolor import colored;\n\nfrom tqdm.notebook import tqdm;\n\nnp.random.seed(10);",
            "mc_idx": 0,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Environment",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 1.0,
                "Data_Extraction": 0.009900990099009901,
                "Exploratory_Data_Analysis": 0.0297029702970297,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.039603960396039604,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    "import ": 10
                },
                "Data_Extraction": {
                    "pandasql": 1
                },
                "Exploratory_Data_Analysis": {
                    "matplotlib": 2,
                    "sns.": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    "sns.": 1,
                    "matplotlib": 2,
                    "pyplot": 1
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    0,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 0,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 0,
                "o_idx": 0
            }
        },
        {
            "source": "# Importing model specific libraries:-\nfrom sklearn_pandas import DataFrameMapper, gen_features;\n\nfrom sklearn.compose import make_column_selector;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.pipeline import make_pipeline, Pipeline ;\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, RobustScaler, OrdinalEncoder;\nfrom sklearn.impute import SimpleImputer;\n\nfrom sklearn.model_selection import KFold, GridSearchCV;\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier;\nfrom xgboost import XGBClassifier;\nfrom lightgbm import LGBMClassifier;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.linear_model import LogisticRegression;\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix;",
            "mc_idx": 1,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Environment",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 1.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.02127659574468085,
                "Data_Transform": 0.0851063829787234,
                "Model_Train": 0.10638297872340426,
                "Model_Evaluation": 0.14184397163120568,
                "Model_Interpretation": 0.028368794326241134,
                "Hyperparameter_Tuning": 0.028368794326241134,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    "import ": 14
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    ".mode": 1
                },
                "Data_Transform": {
                    "transform": 2,
                    "dataframemapper": 1,
                    "labelencoder": 2,
                    "standardscaler": 1,
                    "functiontransformer": 1,
                    "robustscaler": 1,
                    "ordinalencoder": 1,
                    "stack": 1,
                    ".mod": 1,
                    ".compose": 1
                },
                "Model_Train": {
                    "model": 3,
                    "randomforestclassifier": 2,
                    "model_selection": 1,
                    "logisticregression": 1,
                    "gradientboostingclassifier": 1,
                    "decisiontreeclassifier": 1,
                    "svc": 1,
                    "stackingclassifier": 1,
                    "pipeline": 3,
                    ".linear": 1
                },
                "Model_Evaluation": {
                    "confusion_matrix": 2,
                    "accuracy_score": 2,
                    "precision_score": 2,
                    "recall_score": 2,
                    "f1_score": 3,
                    "precision": 1,
                    "recall": 1,
                    "roc_auc_score": 2,
                    "classification_report": 2,
                    "model": 3
                },
                "Model_Interpretation": {
                    "model": 3,
                    "gradient": 1
                },
                "Hyperparameter_Tuning": {
                    "gridsearchcv": 3,
                    "kfold": 1
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    1,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "<IPython.core.display.HTML object>"
                    ]
                },
                "mc_idx": 1,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 1,
                "o_idx": 0
            }
        },
        {
            "source": "# Titanic- Machine Learning from disaster",
            "mc_idx": 2,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# Loading relevant data-sets:-\nxytrain = pd.read_csv('../input/titanic/train.csv', encoding = 'utf8');\nxtest = pd.read_csv('../input/titanic/test.csv', encoding = 'utf8');\n\n# Splitting the training data into features and target:-\nxtrain, ytrain = xytrain.drop('Survived', axis= 1), xytrain[['Survived']];\n\nprint(colored(F\"Train-Test dataframe lengths = {len(xytrain), len(xtest)}\", color= 'blue', attrs= ['bold']));\nprint(colored(F\"\\nTrain-set information\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.info());\nprint(colored(F\"\\nTrain-set description\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.describe().style.format('{:.2f}'));",
            "mc_idx": 3,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Extraction",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 1.0,
                "Exploratory_Data_Analysis": 0.875,
                "Data_Transform": 0.125,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {
                    "read_csv": 4,
                    "pd.read_": 4
                },
                "Exploratory_Data_Analysis": {
                    ".describe(": 1,
                    ".info(": 1,
                    "info": 2,
                    "describe": 1,
                    ".describe": 1,
                    ".info": 1
                },
                "Data_Transform": {
                    ".drop": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    2,
                    3,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[1m\u001b[34mTrain-Test dataframe lengths = (891, 418)\u001b[0m\n\u001b[2m\u001b[1m\u001b[34m\nTrain-set information\n\u001b[0m\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n",
                        "None",
                        "\u001b[2m\u001b[1m\u001b[34m\nTrain-set description\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c036f942a50>"
                    ]
                },
                "mc_idx": 3,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 2,
                "o_idx": 3
            }
        },
        {
            "source": "# 1. Data processing and visualization\n\nWe explore the data set, look into the features, study their distributions, assess nulls and understand the data effectively in this section.\nThese ideas will be used in the next section in the data pipeline.",
            "mc_idx": 4,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### a. Target column details:-",
            "mc_idx": 5,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "plt.subplots(1,1,figsize= (6,6));\nax = ytrain.value_counts().plot.bar(color= 'tab:blue');\nax.set_title(\"Surviver analysis for train set\", color = 'tab:blue', fontsize= 12);\nax.set_xlabel('Survival status', color= 'tab:blue');\nax.set_yticks(range(0, len(xtrain), 50));\nax.set_ylabel('Passengers', color= 'tab:blue');\nplt.xticks(rotation = 0);\nplt.show();",
            "mc_idx": 6,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.2,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.2,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "value_counts()": 1,
                    ".bar(": 1,
                    "size": 2,
                    ".value_counts": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    ".bar(": 1
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    "data/data_Kaggle/images/d0056_c003_o000_image_0.png",
                    3,
                    0,
                    0
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "<Figure size 432x432 with 1 Axes>"
                    ]
                },
                "mc_idx": 6,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 3,
                "o_idx": 0
            }
        },
        {
            "source": "### b. Passenger class and gender:-",
            "mc_idx": 7,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "_ = xytrain.groupby(['Sex', 'Pclass']).agg(Survivors = pd.NamedAgg('Survived', np.sum),Passengers = pd.NamedAgg('Survived', np.size)).sort_index(level=[1,0])\n_['Survival_Rate'] = _['Survivors'] / _['Passengers'];\nprint(colored(f'\\nSurvival rate by gender and pclass\\n', color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format({'Survival_Rate':'{:.2%}'}))\n\n_0 = _.groupby(level= 0).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_0['Survival_Rate'] = _0['Survivors'] / _0['Passengers'];\n\n_1 = _.groupby(level= 1).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_1['Survival_Rate'] = _1['Survivors'] / _1['Passengers'];\n\nprint('\\n');\nfig, ax = plt.subplots(nrows= 1,ncols=2,figsize = (12,6), sharey= True);\nsns.barplot(x = _0.index, y = _0.Survival_Rate, palette = 'Blues', ax = ax[0]);\nsns.barplot(x = _1.index, y = _1.Survival_Rate, palette = 'Blues', ax= ax[1]);\nax[0].set_title(\"Survival analysis by gender\", color = 'tab:blue', fontsize = 12);\nax[1].set_title(\"Survival analysis by passenger class\", color = 'tab:blue', fontsize = 12);\nplt.yticks(np.arange(0,1,0.05),fontsize= 8, color = 'blue');\nplt.show()\n\ndel _, _0, _1;",
            "mc_idx": 8,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0625,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.25,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.1875,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "fig, ax = plt.subplots": 1,
                    "sns.": 2,
                    "size": 5,
                    ".sum": 5,
                    ".groupby": 3
                },
                "Data_Transform": {
                    ".groupby(": 3,
                    ".sort_index": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    "fig, ax = plt.subplots": 1,
                    "sns.": 2
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    "data/data_Kaggle/images/d0056_c004_o003_image_1.png",
                    4,
                    3,
                    1
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[2m\u001b[1m\u001b[34m\nSurvival rate by gender and pclass\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c03271d9fd0>",
                        "\n\n",
                        "<Figure size 864x432 with 2 Axes>"
                    ]
                },
                "mc_idx": 8,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 4,
                "o_idx": 3
            }
        },
        {
            "source": "### c. Age:-",
            "mc_idx": 9,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "fig, ax = plt.subplots(1,2,figsize = (18,6));\nsns.histplot(x = xytrain['Age'], kde= True, palette = 'Blues', ax = ax[0]);\nax[0].set_title(f\"Overall age distribution analysis\", color = 'tab:blue', fontsize= 12 )\n\nsns.boxplot(x = xytrain.Pclass, y = xytrain.Age, palette = 'Blues', ax = ax[1]);\nax[1].set_title(f\"Age distribution per Pclass\", color = 'tab:blue', fontsize= 12);\n\nplt.show();",
            "mc_idx": 10,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.125,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.5,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "fig, ax = plt.subplots": 1,
                    ".boxplot(": 2,
                    "sns.": 2,
                    "size": 3
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    "fig, ax = plt.subplots": 1,
                    ".boxplot(": 1,
                    "sns.": 2
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    "data/data_Kaggle/images/d0056_c005_o000_image_2.png",
                    5,
                    0,
                    2
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "<Figure size 1296x432 with 2 Axes>"
                    ]
                },
                "mc_idx": 10,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 5,
                "o_idx": 0
            }
        },
        {
            "source": "### d. Ticket fare:-",
            "mc_idx": 11,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "print(colored(f\"\\nTicket fare by Pclass and survivorship\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.groupby(['Pclass', 'Survived']).agg({'Fare': [np.amin, np.median, np.mean, np.amax]}).style.format('{:.2f}'))",
            "mc_idx": 12,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.2,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "np.mean": 1,
                    ".median": 2,
                    ".mean": 1,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".groupby(": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    6,
                    1,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[2m\u001b[1m\u001b[34m\nTicket fare by Pclass and survivorship\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c031cd1f790>"
                    ]
                },
                "mc_idx": 12,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 6,
                "o_idx": 1
            }
        },
        {
            "source": "### e. Null valued columns:-",
            "mc_idx": 13,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# Plotting null columns across the data-sets:-\ndef Plot_NullCol(df, df_type):\n    \"\"\"\n    This function plots the relevant data-set and scans for nulls across columns\n    Inputs- \n    df (dataframe):- The relevant data-frame for analysis\n    df_type (string):- Type of data (training/ test)\n    \"\"\";\n    \n    global xtrain;\n    _ = df.isna().sum(axis= 0);\n    print('\\n');\n    \n    plt.subplots(1,1, figsize= (8,8))\n    ax= _.plot.bar(color= 'tab:blue');\n    ax.set_title(f\"Columns with null values in {df_type} data\", color = 'tab:blue', fontsize= 12);\n    ax.set_yticks(range(0, len(xtrain),50));\n    ax.axhline(y= len(xtrain)/4, linewidth = 1.5, color= 'red');\n    ax.set_ylabel('Null values', color = 'tab:blue');\n    ax.set_xlabel('Features', color = 'tab:blue');\n    plt.show();\n    \n    print(colored(f\"Nulls in {df_type} data\\n\", color = 'blue'));\n    display(_);\n    del _;\n    \n\n# Plotting the train-test for nulls:-\nPlot_NullCol(xtrain, df_type= 'train');\nPlot_NullCol(xtest, df_type= 'test');",
            "mc_idx": 14,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.125,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.125,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    ".bar(": 1,
                    "columns": 3,
                    "size": 2,
                    ".isna": 1,
                    ".sum": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    ".bar(": 1
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    7,
                    7,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\n\n",
                        "<Figure size 576x576 with 1 Axes>",
                        "\u001b[34mNulls in train data\n\u001b[0m\n",
                        "PassengerId      0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64",
                        "\n\n",
                        "<Figure size 576x576 with 1 Axes>",
                        "\u001b[34mNulls in test data\n\u001b[0m\n",
                        "PassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64"
                    ]
                },
                "mc_idx": 14,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 7,
                "o_idx": 7
            }
        },
        {
            "source": "### f. Cabin null inference in training set:-",
            "mc_idx": 15,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "print(colored(f\"\\nCabin column null inferences in training data-set\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.assign(Cabin_cat = xtrain.Cabin.str[0:1]).groupby(['Cabin_cat', 'Pclass'], dropna= False).\\\n        agg({'Fare':[np.median, np.mean, np.amin, np.amax], \n             'PassengerId':[np.size], \n             'Survived': [np.sum]})\\\n        .style.format('{:,.0f}'));",
            "mc_idx": 16,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.14285714285714285,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "np.mean": 1,
                    ".median": 2,
                    "size": 1,
                    ".sum": 1,
                    ".mean": 1,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".groupby(": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    8,
                    1,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[2m\u001b[1m\u001b[34m\nCabin column null inferences in training data-set\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c032501d9d0>"
                    ]
                },
                "mc_idx": 16,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 8,
                "o_idx": 1
            }
        },
        {
            "source": "## Creating pipeline adjutant functions and classes:-\n",
            "mc_idx": 17,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "def TreatCabinNulls(df1:pd.DataFrame):\n    \"\"\"\n    This is an added function to treat the null valued cabin column in both the train and test data. \n    This is designed to impute the nulls instead of dropping the column entirely.\n    The treatment of nulls follows the below process-\n    1. Create a composite variable with the cabin category (1st letter in the cabin column) and the Pclass. \n       This is an interaction variable\n    2. Consider passengers with the same Pclass as the subject\n    3. Map the composite Cabin category with the null instances in the cabin based on the lowest fare difference. \n       Windowing SQL functions are used for the same\n    4. For cases where fare is not available, use the mode of the cabin category per Pclass. \n    \n    Input- df1 (dataframe):- Input dataframe without treatment\n    Returns- df (dataframe):- Dataframe with the cabin nulls treated \n    \"\"\";\n    \n    global xtrain, xtest;\n    \n    cabin_trmt_prf = \\\n    sqldf(f\"\"\" \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtrain \n    union all \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtest\n    \"\"\");\n\n    #  Creating the proxy variable with the fare and Pclass:-   \n    cabin_trmt_mst = \\\n    sqldf(f\"\"\"\n    select A1.PID1 as PassengerId, A1.Pclass, A1.Cabin_Ctg_Lbl\n    from \n    (\n    select a.PassengerId as PID1,a.Pclass, b.PassengerId as PID2, b.Cabin_Ctg_Lbl, a.Fare as Fare1, b.Fare as Fare2, abs(a.Fare - b.Fare) as Fare_Diff,\n    row_number() over(partition by a.PassengerId order by abs(a.Fare - b.Fare) asc) as Fare_Diff_Rank\n    from \n    (select PassengerId, Pclass, Fare, Embarked from cabin_trmt_prf WHERE Cabin is null) A \n    inner join \n    (\n    select PassengerId, Pclass, Fare, (cast(Pclass as varchar(1)) || substr(Cabin,1,1)) AS Cabin_Ctg_Lbl, Embarked \n    from cabin_trmt_prf \n    where Cabin is not null\n    ) B on (A.Pclass = B.Pclass and abs(a.Fare - b.Fare) <= 50)\n    ) A1\n    where A1.Fare_Diff_Rank == 1\n    \"\"\");\n    \n    #  Finally appending the nulls still present with the mode of the Pclass and Category label:-\n    cabin_md_sum = \\\n    sqldf(\"\"\"\n    select a.* from \n    (\n    select Pclass, Cabin_Ctg_Lbl, count(PassengerId) as cnt, row_number() over (order by count(PassengerId) desc) as rank_id\n    from cabin_trmt_mst \n    group by Pclass, Cabin_Ctg_Lbl\n    ) a\n    where a.rank_id = 1\n    \"\"\");\n \n    # Mapping the interaction variable to the relevant table:-    \n    df = df1.copy();   \n    df = sqldf(\"\"\"\n    select a.*, coalesce(coalesce(b.Cabin_Ctg_Lbl, cast(a.Pclass as varchar(1)) || substr(a.Cabin,1,1)), c.Cabin_Ctg_Lbl) as Cabin_Class_Lbl \n    from df a \n    left join cabin_trmt_mst b on a.PassengerId = B.PassengerId\n    left join cabin_md_sum c on a.Pclass = c.Pclass\n    \"\"\");  \n    \n    del cabin_trmt_mst, cabin_trmt_prf, cabin_md_sum; \n    return df;  ",
            "mc_idx": 18,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Environment",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 1.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    "variable": 4
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {},
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    9,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 18,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 9,
                "o_idx": 0
            }
        },
        {
            "source": "def Xform_Data(df1:pd.DataFrame): \n    \"\"\"\n    This function does the below tasks:-\n    1. Creates a title attribute from the name column\n    2. Assigns 'others' to uncommon titles\n    3. Creates a flag for 'child' from the title. This is used to fill in the age nulls.\n    4. Creates a 'family members count' column from the Sibsp and Parch attributes\n    5. Drops superfluous columns\n    \n    Input- df (dataframe):- Analysis dataframe\n    Returns- df (dataframe):- Modified dataframe   \n    \"\"\";\n    \n    filterwarnings(action= \"ignore\");\n    df = df1.copy();\n    \n    df['Title'] = df['Name'].apply(lambda x: re.findall(r\"\\S+\\. \",x)[0].strip()[0:-1]);\n    df['Title'].loc[~df.Title.isin(['Mr', 'Mrs', 'Miss', 'Master'])] = 'Others';\n    df['Is_child'] = np.select([df['Title'].str.lower() == 'master'], ['Y'], 'N');    \n    df['Nb_Fmly_Mem'] = df['SibSp'].fillna(0) + df['Parch'].fillna(0);  \n    df = df.drop(['PassengerId', 'Ticket', 'Name', 'Cabin'], axis= 1);  \n    \n    return df;",
            "mc_idx": 19,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.14285714285714285,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {
                    ".fillna(": 2,
                    ".apply(": 1,
                    ".drop": 1,
                    ".fillna": 2,
                    ".apply": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    10,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 19,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 10,
                "o_idx": 0
            }
        },
        {
            "source": "class AgeImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the age null values with the child/ adult gender based medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the child and adult groups for filling nulls\";\n        self.median_ = X.groupby(['Is_child','Sex'])[['Age']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Is_child', 'Sex'], suffixes= ('', '_Median'));\n        X1['Age'] = X1['Age'].fillna(X1.Age_Median);\n        X1 = X1.drop(['Age_Median'], axis= 1);\n        return X1;",
            "mc_idx": 20,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.9,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.2,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    ".median": 6,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".merge(": 1,
                    ".groupby(": 1,
                    ".fillna(": 1,
                    "transform": 3,
                    ".drop": 1,
                    ".fillna": 1,
                    ".reset_index": 1,
                    ".merge": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {
                    "param": 2
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    11,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 20,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 11,
                "o_idx": 0
            }
        },
        {
            "source": "class FareImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the ticket fare null values with the Pclass medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the Pclass groups for filling nulls\";\n        self.median_ = X.groupby(['Pclass'])[['Fare']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Pclass'], suffixes= ('', '_Median'));\n        X1['Fare'] = X1['Fare'].fillna(X1.Fare_Median);\n        X1 = X1.drop(['Fare_Median'], axis= 1);\n        return X1;",
            "mc_idx": 21,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.9,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.2,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    ".median": 6,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".merge(": 1,
                    ".groupby(": 1,
                    ".fillna(": 1,
                    "transform": 3,
                    ".drop": 1,
                    ".fillna": 1,
                    ".reset_index": 1,
                    ".merge": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {
                    "param": 2
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    12,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 21,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 12,
                "o_idx": 0
            }
        },
        {
            "source": "class AgeFareBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class develops bins for the age and ticket fare, to foster stability and offer better and stable predictability.\n    Training set parameters are developed with the fit function. \n    \"\"\";\n    \n    def __init__(self, nb_age_bins: int = 5, nb_fare_bins:int = 5): \n        self.nb_age_bins = nb_age_bins\n        self.nb_fare_bins = nb_fare_bins\n    \n    def fit(self, X, y= None, **fit_params):\n        \"\"\"This function calculates the bins for the age and fare columns respectively\"\"\";     \n        self.age_bins_ = pd.qcut(X['Age'], q = self.nb_age_bins, retbins= True, labels = range(1,self.nb_age_bins + 1,1))[1];\n        self.fare_bins_ = pd.qcut(X['Fare'], q = self.nb_fare_bins, retbins= True, labels = range(1,self.nb_fare_bins + 1,1))[1];   \n        return self;\n        \n    def transform(self, X, y=None, **transform_param):\n        \"\"\"This function applies the binned results to the relevant dataframe and returns the labelled column\"\"\";        \n        X1 = X.copy();\n        \n        self.age_bins_[0] = 0.0; self.fare_bins_[0] = 0.0;\n        self.age_bins_[-1] = np.inf; self.fare_bins_[-1] = np.inf;\n        \n        X1['Age_Bin'] = pd.cut(X1['Age'], self.age_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True);\n        X1['Fare_Bin'] = pd.cut(X1['Fare'], self.fare_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True); \n               \n        global df_col; df_col = list(X1.columns);\n        return X1;      ",
            "mc_idx": 22,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.36363636363636365,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.2727272727272727,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    "columns": 2
                },
                "Data_Transform": {
                    "transform": 3,
                    ".cut(": 2,
                    ".qcut(": 2,
                    ".cut": 2,
                    ".qcut": 2
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {
                    "param": 3
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    13,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 22,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 13,
                "o_idx": 0
            }
        },
        {
            "source": "# 2. Model pipeline development\n\nKey pipeline steps:-\n* Treat nulls in cabin- This step treats nulls in cabin using Pclass and Fare, also fills up remaining nulls after treatment with the Pclass mode\n* Transform data- This step adds the new features and drops some redundant features.\n* Impute Embarked- This step imputes the null values in the column with the mode\n* Label encoder- This is used for the selected object columns only\n* Impute age- This is used to impute age based on the grouped median of child/ adult and gender\n* Impute fare- This fills fare column nulls based on the median of Pclass fare\n* Bin Age Fare- This bins the age and fare columns (this is not used in the latest version)\n* Robust Scaler- This is used for numerical columns only as selected in the column list",
            "mc_idx": 23,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "nb_age_bins = 5;\nnb_fare_bins = 5;\nenc_col_lst = ['Sex', 'Embarked', 'Title', 'Is_child'];\nstd_col_lst = ['Age', 'Fare', 'SibSp', 'Parch', 'Nb_Fmly_Mem' ]\n\nData_Processor = \\\nPipeline(verbose=True, \n         steps= \\\n         [('Imp_Cabin', FunctionTransformer(func= TreatCabinNulls)),\n          ('Xform_Data', FunctionTransformer(func= Xform_Data)),\n          ('Imp_Embk', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                             features= [(['Embarked'], SimpleImputer(strategy= 'most_frequent'))])\n          ),\n          ('Lbl_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                            features = gen_features([col.split(' ') for col in enc_col_lst], [LabelEncoder]))\n          ),\n          ('Imp_Age', AgeImputer()),\n          ('Imp_Fare',FareImputer()),\n          ('Ord_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                      features= [(['Cabin_Class_Lbl'], OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= 49))])\n          ),\n#           ('BinAgeFare', AgeFareBinner(nb_age_bins = nb_age_bins, nb_fare_bins = nb_fare_bins)),\n          ('Std', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                  features = gen_features([col.split(' ') for col in std_col_lst], [RobustScaler])))\n         ]\n        );",
            "mc_idx": 24,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.0,
                "Data_Transform": 1.0,
                "Model_Train": 0.07142857142857142,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {},
                "Data_Transform": {
                    "transform": 2,
                    "dataframemapper": 4,
                    "labelencoder": 2,
                    "functiontransformer": 2,
                    "robustscaler": 1,
                    "ordinalencoder": 1,
                    ".split": 2
                },
                "Model_Train": {
                    "pipeline": 1
                },
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    14,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 24,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 14,
                "o_idx": 0
            }
        },
        {
            "source": "print(colored(f\"\\nPipeline details\\n\", color = 'blue'));\nData_Processor.fit(xtrain, ytrain);\n\n# Train-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the training set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtrain = Data_Processor.transform(xtrain);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtrain.isna().sum(axis=0));\nprint(colored(f'\\nData description after pipeline\\n', color = 'blue'));\ndisplay(Xtrain.describe().style.format('{:.2f}'));\n\n# Test-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the test set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtest = Data_Processor.transform(xtest);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtest.isna().sum(axis=0));",
            "mc_idx": 25,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.5,
                "Model_Train": 0.875,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    ".describe(": 1,
                    "tail": 1,
                    "describe": 1,
                    ".describe": 1,
                    ".isna": 2,
                    ".sum": 2
                },
                "Data_Transform": {
                    "transform": 4
                },
                "Model_Train": {
                    ".fit(": 1,
                    "pipeline": 6
                },
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    15,
                    5,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[34m\nPipeline details\n\u001b[0m\n[Pipeline] ......... (step 1 of 8) Processing Imp_Cabin, total=   0.8s\n[Pipeline] ........ (step 2 of 8) Processing Xform_Data, total=   0.0s\n[Pipeline] .......... (step 3 of 8) Processing Imp_Embk, total=   0.0s\n[Pipeline] ........... (step 4 of 8) Processing Lbl_Enc, total=   0.0s\n[Pipeline] ........... (step 5 of 8) Processing Imp_Age, total=   0.0s\n[Pipeline] .......... (step 6 of 8) Processing Imp_Fare, total=   0.0s\n[Pipeline] ........... (step 7 of 8) Processing Ord_Enc, total=   0.0s\n[Pipeline] ............... (step 8 of 8) Processing Std, total=   0.0s\n\u001b[2m\u001b[1m\u001b[34m\nPipeline implementation for the training set\n\u001b[0m\n\u001b[34m\nNull check after pipeline\n\u001b[0m\n",
                        "Age                0\nFare               0\nSibSp              0\nParch              0\nNb_Fmly_Mem        0\nCabin_Class_Lbl    0\nSex                0\nEmbarked           0\nTitle              0\nIs_child           0\nPclass             0\ndtype: int64",
                        "\u001b[34m\nData description after pipeline\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c031cbe9ad0>",
                        "\u001b[2m\u001b[1m\u001b[34m\nPipeline implementation for the test set\n\u001b[0m\n\u001b[34m\nNull check after pipeline\n\u001b[0m\n",
                        "Age                0\nFare               0\nSibSp              0\nParch              0\nNb_Fmly_Mem        0\nCabin_Class_Lbl    0\nSex                0\nEmbarked           0\nTitle              0\nIs_child           0\nPclass             0\ndtype: int64"
                    ]
                },
                "mc_idx": 25,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 15,
                "o_idx": 5
            }
        },
        {
            "source": "### Model parameter setting and implementation:-\n\nKey models and description:-\n1. Logistic Regression\n2. SVC classifier\n3. Single tree\n4. Ensemble random forest\n5. Gradient boosting machine\n6. Light GBM\n7. XgBoost Classifier\n\nModel outputs are stored in 2 tables- \n1. Model parameters profile (mdl_param_prf)- This stores the model name and relevant score metrics\n2. Model prediction profile (mdl_pred_prf) - This stores the test set predictions from each model (best estimator)",
            "mc_idx": 26,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# Creating a master dictionary for the model parameters and class instances:-\nmdl_mst_dict = \\\n{\n'Logistic': [LogisticRegression(random_state = 10), {}],\n'SVC': [SVC(random_state = 10), {'C': range(3,10,1)}],\n'DTree' : [DecisionTreeClassifier(random_state= 10), {'max_depth': range(4,8,1), 'min_samples_leaf' : range(3,12,1)}],\n'RandomForest': [RandomForestClassifier(random_state =10), \n                 {'n_estimators': range(50,300,25), 'max_depth': range(4,7,1)}],\n'GBM': [GradientBoostingClassifier(random_state= 10), {'max_depth' : range(2,6,1)}],\n'LGBM': [LGBMClassifier(random_state = 10),{}],\n'XgBoost': [XGBClassifier(eval_metric= 'logloss'), {}]\n};\n\ncv = None;\n\n# Creating model output storage objects:-\nmdl_param_prf = pd.DataFrame(data= None, index= list(mdl_mst_dict.keys()), dtype= np.float32,\n                             columns= ['Precision_Score', 'Recall_Score', 'F1_Score', 'ROC_AUC_Score', 'Accuracy_Score']);\nmdl_pred_prf = pd.DataFrame(data= None, index= None, columns= None, dtype= np.int32);",
            "mc_idx": 27,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Model_Evaluation",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.13333333333333333,
                "Data_Transform": 0.0,
                "Model_Train": 0.6,
                "Model_Evaluation": 1.0,
                "Model_Interpretation": 0.2,
                "Hyperparameter_Tuning": 0.13333333333333333,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 2
                },
                "Data_Transform": {},
                "Model_Train": {
                    "model": 2,
                    "randomforestclassifier": 2,
                    "logisticregression": 1,
                    "gradientboostingclassifier": 1,
                    "decisiontreeclassifier": 1,
                    "svc": 2
                },
                "Model_Evaluation": {
                    "accuracy_score": 2,
                    "precision_score": 2,
                    "recall_score": 2,
                    "f1_score": 3,
                    "precision": 1,
                    "recall": 1,
                    "roc_auc_score": 2,
                    "model": 2
                },
                "Model_Interpretation": {
                    "model": 2,
                    "gradient": 1
                },
                "Hyperparameter_Tuning": {
                    "param": 2
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    16,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 27,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 16,
                "o_idx": 0
            }
        },
        {
            "source": "Implementation routine- \n1. Load the relevant model\n2. Fit the model with the grid search parameters\n3. Create relevant scores for the training set\n4. Extract test set predictions\n5. Integrate the data accordingly into the output tables",
            "mc_idx": 28,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# Implementing all models:-\nfor mdl_lbl, mdl_param in tqdm(mdl_mst_dict.items()):\n    print(colored(f\"\\nCurrent model is {mdl_lbl}\", color = 'red', attrs= ['bold', 'dark']));\n    grid = GridSearchCV(estimator = mdl_param[0], param_grid = mdl_param[1], scoring='accuracy', refit=True,cv=cv);\n    grid.fit(Xtrain, ytrain);\n    print(colored(f\"\"\"Best estimator is \\n{grid.best_estimator_}\\n\"\"\", color = 'blue'));\n    \n    ytrain_pred = grid.predict(Xtrain);  \n    print(colored(f\"Confusion matrix\\n{confusion_matrix(ytrain['Survived'].values, ytrain_pred)}\", color = 'blue'));\n    \n    mdl_param_prf.loc[mdl_lbl] = (precision_score(ytrain['Survived'].values, ytrain_pred),\n                                  recall_score(ytrain['Survived'].values, ytrain_pred),\n                                  f1_score(ytrain['Survived'].values, ytrain_pred),\n                                  roc_auc_score(ytrain['Survived'].values, ytrain_pred),\n                                  accuracy_score(ytrain['Survived'].values, ytrain_pred)\n                                 );\n    \n    mdl_pred_prf = pd.concat((mdl_pred_prf,pd.DataFrame(grid.predict(Xtest), index = Xtest.index, columns = [mdl_lbl], dtype= np.int32)),\n                             axis= 1, join='outer');\n\nprint(colored(f\"\\n\\nTraining set scores across models:-\\n\", color = 'blue', attrs= ['bold', 'dark']))\ndisplay(mdl_param_prf.style.format('{:.2%}'));",
            "mc_idx": 29,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Model_Evaluation",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.05,
                "Data_Transform": 0.1,
                "Model_Train": 0.2,
                "Model_Evaluation": 1.0,
                "Model_Interpretation": 0.15,
                "Hyperparameter_Tuning": 0.5,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {
                    ".concat(": 1,
                    ".concat": 1
                },
                "Model_Train": {
                    ".fit(": 1,
                    "model": 3
                },
                "Model_Evaluation": {
                    "confusion_matrix": 2,
                    "accuracy_score": 2,
                    "precision_score": 2,
                    "recall_score": 2,
                    "f1_score": 3,
                    "precision": 1,
                    "recall": 1,
                    "roc_auc_score": 2,
                    "model": 3,
                    ".predict(": 2
                },
                "Model_Interpretation": {
                    "model": 3
                },
                "Hyperparameter_Tuning": {
                    "gridsearchcv": 3,
                    "param_grid": 1,
                    "param": 6
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    17,
                    2,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "  0%|          | 0/7 [00:00<?, ?it/s]",
                        "\u001b[2m\u001b[1m\u001b[31m\nCurrent model is Logistic\u001b[0m\n\u001b[34mBest estimator is \nLogisticRegression(random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[489  60]\n [ 84 258]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is SVC\u001b[0m\n\u001b[34mBest estimator is \nSVC(C=8, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[493  56]\n [ 89 253]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is DTree\u001b[0m\n\u001b[34mBest estimator is \nDecisionTreeClassifier(max_depth=6, min_samples_leaf=6, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[510  39]\n [ 81 261]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is RandomForest\u001b[0m\n\u001b[34mBest estimator is \nRandomForestClassifier(max_depth=6, n_estimators=200, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[524  25]\n [ 89 253]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is GBM\u001b[0m\n\u001b[34mBest estimator is \nGradientBoostingClassifier(max_depth=5, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[541   8]\n [ 29 313]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is LGBM\u001b[0m\n\u001b[34mBest estimator is \nLGBMClassifier(random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[538  11]\n [ 31 311]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is XgBoost\u001b[0m\n\u001b[34mBest estimator is \nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=100, n_jobs=4,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[542   7]\n [ 21 321]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[34m\n\nTraining set scores across models:-\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c0324de58d0>"
                    ]
                },
                "mc_idx": 29,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 17,
                "o_idx": 2
            }
        },
        {
            "source": "# 3. Submission file preparation\n\nWe explore a few options from the developed models and prepare the submissio file in this section. This is the last section in the model framework.",
            "mc_idx": 30,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "print(colored(f\"Sample submission file\\n\", color= 'blue', attrs= ['dark', 'bold']));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').head(5));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').tail(5));",
            "mc_idx": 31,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Extraction",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 1.0,
                "Exploratory_Data_Analysis": 0.75,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {
                    "read_csv": 4,
                    "pd.read_": 4
                },
                "Exploratory_Data_Analysis": {
                    ".head(": 1,
                    ".tail(": 1,
                    "head": 1,
                    "tail": 1,
                    ".head": 1,
                    ".tail": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    18,
                    2,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[1m\u001b[2m\u001b[34mSample submission file\n\u001b[0m\n",
                        "   PassengerId  Survived\n0          892         0\n1          893         1\n2          894         0\n3          895         0\n4          896         1",
                        "     PassengerId  Survived\n413         1305         0\n414         1306         1\n415         1307         0\n416         1308         0\n417         1309         0"
                    ]
                },
                "mc_idx": 31,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 18,
                "o_idx": 2
            }
        },
        {
            "source": "# Adding new columns for specific mode values:-\nmdl_pred_prf['AllModels'] = mode(mdl_pred_prf, axis=1)[0];\nmdl_pred_prf['BoostedTree'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost']], axis=1)[0];\nmdl_pred_prf['Ensemble'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost', 'RandomForest']], axis=1)[0];",
            "mc_idx": 32,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 1.0,
                "Model_Evaluation": 1.0,
                "Model_Interpretation": 1.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {},
                "Model_Train": {
                    "model": 1
                },
                "Model_Evaluation": {
                    "model": 1
                },
                "Model_Interpretation": {
                    "model": 1
                },
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    19,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 32,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 19,
                "o_idx": 0
            }
        },
        {
            "source": "sel_col_nm = 'AllModels';\npd.DataFrame(mdl_pred_prf[sel_col_nm].values, index = xtest.PassengerId, \n             columns = ['Survived'], dtype= np.int32).reset_index().to_csv(\"Submission.csv\", index= False);",
            "mc_idx": 33,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Export",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.5,
                "Data_Transform": 0.5,
                "Model_Train": 0.5,
                "Model_Evaluation": 0.5,
                "Model_Interpretation": 0.5,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 1.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {
                    ".reset_index": 1
                },
                "Model_Train": {
                    "model": 1
                },
                "Model_Evaluation": {
                    "model": 1
                },
                "Model_Interpretation": {
                    "model": 1
                },
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {
                    ".to_csv(": 1,
                    "to_csv": 1
                },
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    20,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 33,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 20,
                "o_idx": 0
            }
        }
    ],
    "code_cells": [
        {
            "source": "# Importing general libraries:-\n\nimport numpy as np; \nfrom scipy.stats import mode;\nimport pandas as pd;\nfrom pandasql import sqldf;\nimport regex as re;\n\nimport matplotlib.pyplot as plt; \n%matplotlib inline\nimport seaborn as sns;\nsns.set_style('darkgrid');\n\nfrom warnings import filterwarnings;\nfrom termcolor import colored;\n\nfrom tqdm.notebook import tqdm;\n\nnp.random.seed(10);",
            "mc_idx": 0,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Environment",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 1.0,
                "Data_Extraction": 0.009900990099009901,
                "Exploratory_Data_Analysis": 0.0297029702970297,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.039603960396039604,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    "import ": 10
                },
                "Data_Extraction": {
                    "pandasql": 1
                },
                "Exploratory_Data_Analysis": {
                    "matplotlib": 2,
                    "sns.": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    "sns.": 1,
                    "matplotlib": 2,
                    "pyplot": 1
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    0,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 0,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 0,
                "o_idx": 0
            }
        },
        {
            "source": "# Importing model specific libraries:-\nfrom sklearn_pandas import DataFrameMapper, gen_features;\n\nfrom sklearn.compose import make_column_selector;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.pipeline import make_pipeline, Pipeline ;\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, RobustScaler, OrdinalEncoder;\nfrom sklearn.impute import SimpleImputer;\n\nfrom sklearn.model_selection import KFold, GridSearchCV;\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier;\nfrom xgboost import XGBClassifier;\nfrom lightgbm import LGBMClassifier;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.linear_model import LogisticRegression;\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix;",
            "mc_idx": 1,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Environment",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 1.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.02127659574468085,
                "Data_Transform": 0.0851063829787234,
                "Model_Train": 0.10638297872340426,
                "Model_Evaluation": 0.14184397163120568,
                "Model_Interpretation": 0.028368794326241134,
                "Hyperparameter_Tuning": 0.028368794326241134,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    "import ": 14
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    ".mode": 1
                },
                "Data_Transform": {
                    "transform": 2,
                    "dataframemapper": 1,
                    "labelencoder": 2,
                    "standardscaler": 1,
                    "functiontransformer": 1,
                    "robustscaler": 1,
                    "ordinalencoder": 1,
                    "stack": 1,
                    ".mod": 1,
                    ".compose": 1
                },
                "Model_Train": {
                    "model": 3,
                    "randomforestclassifier": 2,
                    "model_selection": 1,
                    "logisticregression": 1,
                    "gradientboostingclassifier": 1,
                    "decisiontreeclassifier": 1,
                    "svc": 1,
                    "stackingclassifier": 1,
                    "pipeline": 3,
                    ".linear": 1
                },
                "Model_Evaluation": {
                    "confusion_matrix": 2,
                    "accuracy_score": 2,
                    "precision_score": 2,
                    "recall_score": 2,
                    "f1_score": 3,
                    "precision": 1,
                    "recall": 1,
                    "roc_auc_score": 2,
                    "classification_report": 2,
                    "model": 3
                },
                "Model_Interpretation": {
                    "model": 3,
                    "gradient": 1
                },
                "Hyperparameter_Tuning": {
                    "gridsearchcv": 3,
                    "kfold": 1
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    1,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "<IPython.core.display.HTML object>"
                    ]
                },
                "mc_idx": 1,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 1,
                "o_idx": 0
            }
        },
        {
            "source": "# Loading relevant data-sets:-\nxytrain = pd.read_csv('../input/titanic/train.csv', encoding = 'utf8');\nxtest = pd.read_csv('../input/titanic/test.csv', encoding = 'utf8');\n\n# Splitting the training data into features and target:-\nxtrain, ytrain = xytrain.drop('Survived', axis= 1), xytrain[['Survived']];\n\nprint(colored(F\"Train-Test dataframe lengths = {len(xytrain), len(xtest)}\", color= 'blue', attrs= ['bold']));\nprint(colored(F\"\\nTrain-set information\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.info());\nprint(colored(F\"\\nTrain-set description\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.describe().style.format('{:.2f}'));",
            "mc_idx": 3,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Extraction",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 1.0,
                "Exploratory_Data_Analysis": 0.875,
                "Data_Transform": 0.125,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {
                    "read_csv": 4,
                    "pd.read_": 4
                },
                "Exploratory_Data_Analysis": {
                    ".describe(": 1,
                    ".info(": 1,
                    "info": 2,
                    "describe": 1,
                    ".describe": 1,
                    ".info": 1
                },
                "Data_Transform": {
                    ".drop": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    2,
                    3,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[1m\u001b[34mTrain-Test dataframe lengths = (891, 418)\u001b[0m\n\u001b[2m\u001b[1m\u001b[34m\nTrain-set information\n\u001b[0m\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n",
                        "None",
                        "\u001b[2m\u001b[1m\u001b[34m\nTrain-set description\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c036f942a50>"
                    ]
                },
                "mc_idx": 3,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 2,
                "o_idx": 3
            }
        },
        {
            "source": "plt.subplots(1,1,figsize= (6,6));\nax = ytrain.value_counts().plot.bar(color= 'tab:blue');\nax.set_title(\"Surviver analysis for train set\", color = 'tab:blue', fontsize= 12);\nax.set_xlabel('Survival status', color= 'tab:blue');\nax.set_yticks(range(0, len(xtrain), 50));\nax.set_ylabel('Passengers', color= 'tab:blue');\nplt.xticks(rotation = 0);\nplt.show();",
            "mc_idx": 6,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.2,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.2,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "value_counts()": 1,
                    ".bar(": 1,
                    "size": 2,
                    ".value_counts": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    ".bar(": 1
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    "data/data_Kaggle/images/d0056_c003_o000_image_0.png",
                    3,
                    0,
                    0
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "<Figure size 432x432 with 1 Axes>"
                    ]
                },
                "mc_idx": 6,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 3,
                "o_idx": 0
            }
        },
        {
            "source": "_ = xytrain.groupby(['Sex', 'Pclass']).agg(Survivors = pd.NamedAgg('Survived', np.sum),Passengers = pd.NamedAgg('Survived', np.size)).sort_index(level=[1,0])\n_['Survival_Rate'] = _['Survivors'] / _['Passengers'];\nprint(colored(f'\\nSurvival rate by gender and pclass\\n', color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format({'Survival_Rate':'{:.2%}'}))\n\n_0 = _.groupby(level= 0).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_0['Survival_Rate'] = _0['Survivors'] / _0['Passengers'];\n\n_1 = _.groupby(level= 1).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_1['Survival_Rate'] = _1['Survivors'] / _1['Passengers'];\n\nprint('\\n');\nfig, ax = plt.subplots(nrows= 1,ncols=2,figsize = (12,6), sharey= True);\nsns.barplot(x = _0.index, y = _0.Survival_Rate, palette = 'Blues', ax = ax[0]);\nsns.barplot(x = _1.index, y = _1.Survival_Rate, palette = 'Blues', ax= ax[1]);\nax[0].set_title(\"Survival analysis by gender\", color = 'tab:blue', fontsize = 12);\nax[1].set_title(\"Survival analysis by passenger class\", color = 'tab:blue', fontsize = 12);\nplt.yticks(np.arange(0,1,0.05),fontsize= 8, color = 'blue');\nplt.show()\n\ndel _, _0, _1;",
            "mc_idx": 8,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0625,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.25,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.1875,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "fig, ax = plt.subplots": 1,
                    "sns.": 2,
                    "size": 5,
                    ".sum": 5,
                    ".groupby": 3
                },
                "Data_Transform": {
                    ".groupby(": 3,
                    ".sort_index": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    "fig, ax = plt.subplots": 1,
                    "sns.": 2
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    "data/data_Kaggle/images/d0056_c004_o003_image_1.png",
                    4,
                    3,
                    1
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[2m\u001b[1m\u001b[34m\nSurvival rate by gender and pclass\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c03271d9fd0>",
                        "\n\n",
                        "<Figure size 864x432 with 2 Axes>"
                    ]
                },
                "mc_idx": 8,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 4,
                "o_idx": 3
            }
        },
        {
            "source": "fig, ax = plt.subplots(1,2,figsize = (18,6));\nsns.histplot(x = xytrain['Age'], kde= True, palette = 'Blues', ax = ax[0]);\nax[0].set_title(f\"Overall age distribution analysis\", color = 'tab:blue', fontsize= 12 )\n\nsns.boxplot(x = xytrain.Pclass, y = xytrain.Age, palette = 'Blues', ax = ax[1]);\nax[1].set_title(f\"Age distribution per Pclass\", color = 'tab:blue', fontsize= 12);\n\nplt.show();",
            "mc_idx": 10,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.125,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.5,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "fig, ax = plt.subplots": 1,
                    ".boxplot(": 2,
                    "sns.": 2,
                    "size": 3
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    "fig, ax = plt.subplots": 1,
                    ".boxplot(": 1,
                    "sns.": 2
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    "data/data_Kaggle/images/d0056_c005_o000_image_2.png",
                    5,
                    0,
                    2
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "<Figure size 1296x432 with 2 Axes>"
                    ]
                },
                "mc_idx": 10,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 5,
                "o_idx": 0
            }
        },
        {
            "source": "print(colored(f\"\\nTicket fare by Pclass and survivorship\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.groupby(['Pclass', 'Survived']).agg({'Fare': [np.amin, np.median, np.mean, np.amax]}).style.format('{:.2f}'))",
            "mc_idx": 12,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.2,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "np.mean": 1,
                    ".median": 2,
                    ".mean": 1,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".groupby(": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    6,
                    1,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[2m\u001b[1m\u001b[34m\nTicket fare by Pclass and survivorship\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c031cd1f790>"
                    ]
                },
                "mc_idx": 12,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 6,
                "o_idx": 1
            }
        },
        {
            "source": "# Plotting null columns across the data-sets:-\ndef Plot_NullCol(df, df_type):\n    \"\"\"\n    This function plots the relevant data-set and scans for nulls across columns\n    Inputs- \n    df (dataframe):- The relevant data-frame for analysis\n    df_type (string):- Type of data (training/ test)\n    \"\"\";\n    \n    global xtrain;\n    _ = df.isna().sum(axis= 0);\n    print('\\n');\n    \n    plt.subplots(1,1, figsize= (8,8))\n    ax= _.plot.bar(color= 'tab:blue');\n    ax.set_title(f\"Columns with null values in {df_type} data\", color = 'tab:blue', fontsize= 12);\n    ax.set_yticks(range(0, len(xtrain),50));\n    ax.axhline(y= len(xtrain)/4, linewidth = 1.5, color= 'red');\n    ax.set_ylabel('Null values', color = 'tab:blue');\n    ax.set_xlabel('Features', color = 'tab:blue');\n    plt.show();\n    \n    print(colored(f\"Nulls in {df_type} data\\n\", color = 'blue'));\n    display(_);\n    del _;\n    \n\n# Plotting the train-test for nulls:-\nPlot_NullCol(xtrain, df_type= 'train');\nPlot_NullCol(xtest, df_type= 'test');",
            "mc_idx": 14,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.125,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.125,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    ".show": 1
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    ".bar(": 1,
                    "columns": 3,
                    "size": 2,
                    ".isna": 1,
                    ".sum": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {
                    ".bar(": 1
                },
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    7,
                    7,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\n\n",
                        "<Figure size 576x576 with 1 Axes>",
                        "\u001b[34mNulls in train data\n\u001b[0m\n",
                        "PassengerId      0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64",
                        "\n\n",
                        "<Figure size 576x576 with 1 Axes>",
                        "\u001b[34mNulls in test data\n\u001b[0m\n",
                        "PassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64"
                    ]
                },
                "mc_idx": 14,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 7,
                "o_idx": 7
            }
        },
        {
            "source": "print(colored(f\"\\nCabin column null inferences in training data-set\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.assign(Cabin_cat = xtrain.Cabin.str[0:1]).groupby(['Cabin_cat', 'Pclass'], dropna= False).\\\n        agg({'Fare':[np.median, np.mean, np.amin, np.amax], \n             'PassengerId':[np.size], \n             'Survived': [np.sum]})\\\n        .style.format('{:,.0f}'));",
            "mc_idx": 16,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.14285714285714285,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "np.mean": 1,
                    ".median": 2,
                    "size": 1,
                    ".sum": 1,
                    ".mean": 1,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".groupby(": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    8,
                    1,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[2m\u001b[1m\u001b[34m\nCabin column null inferences in training data-set\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c032501d9d0>"
                    ]
                },
                "mc_idx": 16,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 8,
                "o_idx": 1
            }
        },
        {
            "source": "def TreatCabinNulls(df1:pd.DataFrame):\n    \"\"\"\n    This is an added function to treat the null valued cabin column in both the train and test data. \n    This is designed to impute the nulls instead of dropping the column entirely.\n    The treatment of nulls follows the below process-\n    1. Create a composite variable with the cabin category (1st letter in the cabin column) and the Pclass. \n       This is an interaction variable\n    2. Consider passengers with the same Pclass as the subject\n    3. Map the composite Cabin category with the null instances in the cabin based on the lowest fare difference. \n       Windowing SQL functions are used for the same\n    4. For cases where fare is not available, use the mode of the cabin category per Pclass. \n    \n    Input- df1 (dataframe):- Input dataframe without treatment\n    Returns- df (dataframe):- Dataframe with the cabin nulls treated \n    \"\"\";\n    \n    global xtrain, xtest;\n    \n    cabin_trmt_prf = \\\n    sqldf(f\"\"\" \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtrain \n    union all \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtest\n    \"\"\");\n\n    #  Creating the proxy variable with the fare and Pclass:-   \n    cabin_trmt_mst = \\\n    sqldf(f\"\"\"\n    select A1.PID1 as PassengerId, A1.Pclass, A1.Cabin_Ctg_Lbl\n    from \n    (\n    select a.PassengerId as PID1,a.Pclass, b.PassengerId as PID2, b.Cabin_Ctg_Lbl, a.Fare as Fare1, b.Fare as Fare2, abs(a.Fare - b.Fare) as Fare_Diff,\n    row_number() over(partition by a.PassengerId order by abs(a.Fare - b.Fare) asc) as Fare_Diff_Rank\n    from \n    (select PassengerId, Pclass, Fare, Embarked from cabin_trmt_prf WHERE Cabin is null) A \n    inner join \n    (\n    select PassengerId, Pclass, Fare, (cast(Pclass as varchar(1)) || substr(Cabin,1,1)) AS Cabin_Ctg_Lbl, Embarked \n    from cabin_trmt_prf \n    where Cabin is not null\n    ) B on (A.Pclass = B.Pclass and abs(a.Fare - b.Fare) <= 50)\n    ) A1\n    where A1.Fare_Diff_Rank == 1\n    \"\"\");\n    \n    #  Finally appending the nulls still present with the mode of the Pclass and Category label:-\n    cabin_md_sum = \\\n    sqldf(\"\"\"\n    select a.* from \n    (\n    select Pclass, Cabin_Ctg_Lbl, count(PassengerId) as cnt, row_number() over (order by count(PassengerId) desc) as rank_id\n    from cabin_trmt_mst \n    group by Pclass, Cabin_Ctg_Lbl\n    ) a\n    where a.rank_id = 1\n    \"\"\");\n \n    # Mapping the interaction variable to the relevant table:-    \n    df = df1.copy();   \n    df = sqldf(\"\"\"\n    select a.*, coalesce(coalesce(b.Cabin_Ctg_Lbl, cast(a.Pclass as varchar(1)) || substr(a.Cabin,1,1)), c.Cabin_Ctg_Lbl) as Cabin_Class_Lbl \n    from df a \n    left join cabin_trmt_mst b on a.PassengerId = B.PassengerId\n    left join cabin_md_sum c on a.Pclass = c.Pclass\n    \"\"\");  \n    \n    del cabin_trmt_mst, cabin_trmt_prf, cabin_md_sum; \n    return df;  ",
            "mc_idx": 18,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Environment",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 1.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.0,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {
                    "variable": 4
                },
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {},
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    9,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 18,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 9,
                "o_idx": 0
            }
        },
        {
            "source": "def Xform_Data(df1:pd.DataFrame): \n    \"\"\"\n    This function does the below tasks:-\n    1. Creates a title attribute from the name column\n    2. Assigns 'others' to uncommon titles\n    3. Creates a flag for 'child' from the title. This is used to fill in the age nulls.\n    4. Creates a 'family members count' column from the Sibsp and Parch attributes\n    5. Drops superfluous columns\n    \n    Input- df (dataframe):- Analysis dataframe\n    Returns- df (dataframe):- Modified dataframe   \n    \"\"\";\n    \n    filterwarnings(action= \"ignore\");\n    df = df1.copy();\n    \n    df['Title'] = df['Name'].apply(lambda x: re.findall(r\"\\S+\\. \",x)[0].strip()[0:-1]);\n    df['Title'].loc[~df.Title.isin(['Mr', 'Mrs', 'Miss', 'Master'])] = 'Others';\n    df['Is_child'] = np.select([df['Title'].str.lower() == 'master'], ['Y'], 'N');    \n    df['Nb_Fmly_Mem'] = df['SibSp'].fillna(0) + df['Parch'].fillna(0);  \n    df = df.drop(['PassengerId', 'Ticket', 'Name', 'Cabin'], axis= 1);  \n    \n    return df;",
            "mc_idx": 19,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.14285714285714285,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {
                    ".fillna(": 2,
                    ".apply(": 1,
                    ".drop": 1,
                    ".fillna": 2,
                    ".apply": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    10,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 19,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 10,
                "o_idx": 0
            }
        },
        {
            "source": "class AgeImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the age null values with the child/ adult gender based medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the child and adult groups for filling nulls\";\n        self.median_ = X.groupby(['Is_child','Sex'])[['Age']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Is_child', 'Sex'], suffixes= ('', '_Median'));\n        X1['Age'] = X1['Age'].fillna(X1.Age_Median);\n        X1 = X1.drop(['Age_Median'], axis= 1);\n        return X1;",
            "mc_idx": 20,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.9,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.2,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    ".median": 6,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".merge(": 1,
                    ".groupby(": 1,
                    ".fillna(": 1,
                    "transform": 3,
                    ".drop": 1,
                    ".fillna": 1,
                    ".reset_index": 1,
                    ".merge": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {
                    "param": 2
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    11,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 20,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 11,
                "o_idx": 0
            }
        },
        {
            "source": "class FareImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the ticket fare null values with the Pclass medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the Pclass groups for filling nulls\";\n        self.median_ = X.groupby(['Pclass'])[['Fare']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Pclass'], suffixes= ('', '_Median'));\n        X1['Fare'] = X1['Fare'].fillna(X1.Fare_Median);\n        X1 = X1.drop(['Fare_Median'], axis= 1);\n        return X1;",
            "mc_idx": 21,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.9,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.2,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    ".median": 6,
                    ".groupby": 1
                },
                "Data_Transform": {
                    ".merge(": 1,
                    ".groupby(": 1,
                    ".fillna(": 1,
                    "transform": 3,
                    ".drop": 1,
                    ".fillna": 1,
                    ".reset_index": 1,
                    ".merge": 1
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {
                    "param": 2
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    12,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 21,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 12,
                "o_idx": 0
            }
        },
        {
            "source": "class AgeFareBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class develops bins for the age and ticket fare, to foster stability and offer better and stable predictability.\n    Training set parameters are developed with the fit function. \n    \"\"\";\n    \n    def __init__(self, nb_age_bins: int = 5, nb_fare_bins:int = 5): \n        self.nb_age_bins = nb_age_bins\n        self.nb_fare_bins = nb_fare_bins\n    \n    def fit(self, X, y= None, **fit_params):\n        \"\"\"This function calculates the bins for the age and fare columns respectively\"\"\";     \n        self.age_bins_ = pd.qcut(X['Age'], q = self.nb_age_bins, retbins= True, labels = range(1,self.nb_age_bins + 1,1))[1];\n        self.fare_bins_ = pd.qcut(X['Fare'], q = self.nb_fare_bins, retbins= True, labels = range(1,self.nb_fare_bins + 1,1))[1];   \n        return self;\n        \n    def transform(self, X, y=None, **transform_param):\n        \"\"\"This function applies the binned results to the relevant dataframe and returns the labelled column\"\"\";        \n        X1 = X.copy();\n        \n        self.age_bins_[0] = 0.0; self.fare_bins_[0] = 0.0;\n        self.age_bins_[-1] = np.inf; self.fare_bins_[-1] = np.inf;\n        \n        X1['Age_Bin'] = pd.cut(X1['Age'], self.age_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True);\n        X1['Fare_Bin'] = pd.cut(X1['Fare'], self.fare_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True); \n               \n        global df_col; df_col = list(X1.columns);\n        return X1;      ",
            "mc_idx": 22,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.36363636363636365,
                "Data_Transform": 1.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.2727272727272727,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "baseestimator": 1,
                    "transformermixin": 1,
                    "columns": 2
                },
                "Data_Transform": {
                    "transform": 3,
                    ".cut(": 2,
                    ".qcut(": 2,
                    ".cut": 2,
                    ".qcut": 2
                },
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {
                    "param": 3
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    13,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 22,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 13,
                "o_idx": 0
            }
        },
        {
            "source": "nb_age_bins = 5;\nnb_fare_bins = 5;\nenc_col_lst = ['Sex', 'Embarked', 'Title', 'Is_child'];\nstd_col_lst = ['Age', 'Fare', 'SibSp', 'Parch', 'Nb_Fmly_Mem' ]\n\nData_Processor = \\\nPipeline(verbose=True, \n         steps= \\\n         [('Imp_Cabin', FunctionTransformer(func= TreatCabinNulls)),\n          ('Xform_Data', FunctionTransformer(func= Xform_Data)),\n          ('Imp_Embk', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                             features= [(['Embarked'], SimpleImputer(strategy= 'most_frequent'))])\n          ),\n          ('Lbl_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                            features = gen_features([col.split(' ') for col in enc_col_lst], [LabelEncoder]))\n          ),\n          ('Imp_Age', AgeImputer()),\n          ('Imp_Fare',FareImputer()),\n          ('Ord_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                      features= [(['Cabin_Class_Lbl'], OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= 49))])\n          ),\n#           ('BinAgeFare', AgeFareBinner(nb_age_bins = nb_age_bins, nb_fare_bins = nb_fare_bins)),\n          ('Std', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                  features = gen_features([col.split(' ') for col in std_col_lst], [RobustScaler])))\n         ]\n        );",
            "mc_idx": 24,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Transform",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.0,
                "Data_Transform": 1.0,
                "Model_Train": 0.07142857142857142,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {},
                "Data_Transform": {
                    "transform": 2,
                    "dataframemapper": 4,
                    "labelencoder": 2,
                    "functiontransformer": 2,
                    "robustscaler": 1,
                    "ordinalencoder": 1,
                    ".split": 2
                },
                "Model_Train": {
                    "pipeline": 1
                },
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    14,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 24,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 14,
                "o_idx": 0
            }
        },
        {
            "source": "print(colored(f\"\\nPipeline details\\n\", color = 'blue'));\nData_Processor.fit(xtrain, ytrain);\n\n# Train-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the training set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtrain = Data_Processor.transform(xtrain);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtrain.isna().sum(axis=0));\nprint(colored(f'\\nData description after pipeline\\n', color = 'blue'));\ndisplay(Xtrain.describe().style.format('{:.2f}'));\n\n# Test-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the test set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtest = Data_Processor.transform(xtest);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtest.isna().sum(axis=0));",
            "mc_idx": 25,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.5,
                "Model_Train": 0.875,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    ".describe(": 1,
                    "tail": 1,
                    "describe": 1,
                    ".describe": 1,
                    ".isna": 2,
                    ".sum": 2
                },
                "Data_Transform": {
                    "transform": 4
                },
                "Model_Train": {
                    ".fit(": 1,
                    "pipeline": 6
                },
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    15,
                    5,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[34m\nPipeline details\n\u001b[0m\n[Pipeline] ......... (step 1 of 8) Processing Imp_Cabin, total=   0.8s\n[Pipeline] ........ (step 2 of 8) Processing Xform_Data, total=   0.0s\n[Pipeline] .......... (step 3 of 8) Processing Imp_Embk, total=   0.0s\n[Pipeline] ........... (step 4 of 8) Processing Lbl_Enc, total=   0.0s\n[Pipeline] ........... (step 5 of 8) Processing Imp_Age, total=   0.0s\n[Pipeline] .......... (step 6 of 8) Processing Imp_Fare, total=   0.0s\n[Pipeline] ........... (step 7 of 8) Processing Ord_Enc, total=   0.0s\n[Pipeline] ............... (step 8 of 8) Processing Std, total=   0.0s\n\u001b[2m\u001b[1m\u001b[34m\nPipeline implementation for the training set\n\u001b[0m\n\u001b[34m\nNull check after pipeline\n\u001b[0m\n",
                        "Age                0\nFare               0\nSibSp              0\nParch              0\nNb_Fmly_Mem        0\nCabin_Class_Lbl    0\nSex                0\nEmbarked           0\nTitle              0\nIs_child           0\nPclass             0\ndtype: int64",
                        "\u001b[34m\nData description after pipeline\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c031cbe9ad0>",
                        "\u001b[2m\u001b[1m\u001b[34m\nPipeline implementation for the test set\n\u001b[0m\n\u001b[34m\nNull check after pipeline\n\u001b[0m\n",
                        "Age                0\nFare               0\nSibSp              0\nParch              0\nNb_Fmly_Mem        0\nCabin_Class_Lbl    0\nSex                0\nEmbarked           0\nTitle              0\nIs_child           0\nPclass             0\ndtype: int64"
                    ]
                },
                "mc_idx": 25,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 15,
                "o_idx": 5
            }
        },
        {
            "source": "# Creating a master dictionary for the model parameters and class instances:-\nmdl_mst_dict = \\\n{\n'Logistic': [LogisticRegression(random_state = 10), {}],\n'SVC': [SVC(random_state = 10), {'C': range(3,10,1)}],\n'DTree' : [DecisionTreeClassifier(random_state= 10), {'max_depth': range(4,8,1), 'min_samples_leaf' : range(3,12,1)}],\n'RandomForest': [RandomForestClassifier(random_state =10), \n                 {'n_estimators': range(50,300,25), 'max_depth': range(4,7,1)}],\n'GBM': [GradientBoostingClassifier(random_state= 10), {'max_depth' : range(2,6,1)}],\n'LGBM': [LGBMClassifier(random_state = 10),{}],\n'XgBoost': [XGBClassifier(eval_metric= 'logloss'), {}]\n};\n\ncv = None;\n\n# Creating model output storage objects:-\nmdl_param_prf = pd.DataFrame(data= None, index= list(mdl_mst_dict.keys()), dtype= np.float32,\n                             columns= ['Precision_Score', 'Recall_Score', 'F1_Score', 'ROC_AUC_Score', 'Accuracy_Score']);\nmdl_pred_prf = pd.DataFrame(data= None, index= None, columns= None, dtype= np.int32);",
            "mc_idx": 27,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Model_Evaluation",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.13333333333333333,
                "Data_Transform": 0.0,
                "Model_Train": 0.6,
                "Model_Evaluation": 1.0,
                "Model_Interpretation": 0.2,
                "Hyperparameter_Tuning": 0.13333333333333333,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 2
                },
                "Data_Transform": {},
                "Model_Train": {
                    "model": 2,
                    "randomforestclassifier": 2,
                    "logisticregression": 1,
                    "gradientboostingclassifier": 1,
                    "decisiontreeclassifier": 1,
                    "svc": 2
                },
                "Model_Evaluation": {
                    "accuracy_score": 2,
                    "precision_score": 2,
                    "recall_score": 2,
                    "f1_score": 3,
                    "precision": 1,
                    "recall": 1,
                    "roc_auc_score": 2,
                    "model": 2
                },
                "Model_Interpretation": {
                    "model": 2,
                    "gradient": 1
                },
                "Hyperparameter_Tuning": {
                    "param": 2
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    16,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 27,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 16,
                "o_idx": 0
            }
        },
        {
            "source": "# Implementing all models:-\nfor mdl_lbl, mdl_param in tqdm(mdl_mst_dict.items()):\n    print(colored(f\"\\nCurrent model is {mdl_lbl}\", color = 'red', attrs= ['bold', 'dark']));\n    grid = GridSearchCV(estimator = mdl_param[0], param_grid = mdl_param[1], scoring='accuracy', refit=True,cv=cv);\n    grid.fit(Xtrain, ytrain);\n    print(colored(f\"\"\"Best estimator is \\n{grid.best_estimator_}\\n\"\"\", color = 'blue'));\n    \n    ytrain_pred = grid.predict(Xtrain);  \n    print(colored(f\"Confusion matrix\\n{confusion_matrix(ytrain['Survived'].values, ytrain_pred)}\", color = 'blue'));\n    \n    mdl_param_prf.loc[mdl_lbl] = (precision_score(ytrain['Survived'].values, ytrain_pred),\n                                  recall_score(ytrain['Survived'].values, ytrain_pred),\n                                  f1_score(ytrain['Survived'].values, ytrain_pred),\n                                  roc_auc_score(ytrain['Survived'].values, ytrain_pred),\n                                  accuracy_score(ytrain['Survived'].values, ytrain_pred)\n                                 );\n    \n    mdl_pred_prf = pd.concat((mdl_pred_prf,pd.DataFrame(grid.predict(Xtest), index = Xtest.index, columns = [mdl_lbl], dtype= np.int32)),\n                             axis= 1, join='outer');\n\nprint(colored(f\"\\n\\nTraining set scores across models:-\\n\", color = 'blue', attrs= ['bold', 'dark']))\ndisplay(mdl_param_prf.style.format('{:.2%}'));",
            "mc_idx": 29,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Model_Evaluation",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.05,
                "Data_Transform": 0.1,
                "Model_Train": 0.2,
                "Model_Evaluation": 1.0,
                "Model_Interpretation": 0.15,
                "Hyperparameter_Tuning": 0.5,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {
                    ".concat(": 1,
                    ".concat": 1
                },
                "Model_Train": {
                    ".fit(": 1,
                    "model": 3
                },
                "Model_Evaluation": {
                    "confusion_matrix": 2,
                    "accuracy_score": 2,
                    "precision_score": 2,
                    "recall_score": 2,
                    "f1_score": 3,
                    "precision": 1,
                    "recall": 1,
                    "roc_auc_score": 2,
                    "model": 3,
                    ".predict(": 2
                },
                "Model_Interpretation": {
                    "model": 3
                },
                "Hyperparameter_Tuning": {
                    "gridsearchcv": 3,
                    "param_grid": 1,
                    "param": 6
                },
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    17,
                    2,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "  0%|          | 0/7 [00:00<?, ?it/s]",
                        "\u001b[2m\u001b[1m\u001b[31m\nCurrent model is Logistic\u001b[0m\n\u001b[34mBest estimator is \nLogisticRegression(random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[489  60]\n [ 84 258]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is SVC\u001b[0m\n\u001b[34mBest estimator is \nSVC(C=8, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[493  56]\n [ 89 253]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is DTree\u001b[0m\n\u001b[34mBest estimator is \nDecisionTreeClassifier(max_depth=6, min_samples_leaf=6, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[510  39]\n [ 81 261]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is RandomForest\u001b[0m\n\u001b[34mBest estimator is \nRandomForestClassifier(max_depth=6, n_estimators=200, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[524  25]\n [ 89 253]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is GBM\u001b[0m\n\u001b[34mBest estimator is \nGradientBoostingClassifier(max_depth=5, random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[541   8]\n [ 29 313]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is LGBM\u001b[0m\n\u001b[34mBest estimator is \nLGBMClassifier(random_state=10)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[538  11]\n [ 31 311]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[31m\nCurrent model is XgBoost\u001b[0m\n\u001b[34mBest estimator is \nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=100, n_jobs=4,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\u001b[0m\n\u001b[34mConfusion matrix\n[[542   7]\n [ 21 321]]\u001b[0m\n\u001b[2m\u001b[1m\u001b[34m\n\nTraining set scores across models:-\n\u001b[0m\n",
                        "<pandas.io.formats.style.Styler at 0x7c0324de58d0>"
                    ]
                },
                "mc_idx": 29,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 17,
                "o_idx": 2
            }
        },
        {
            "source": "print(colored(f\"Sample submission file\\n\", color= 'blue', attrs= ['dark', 'bold']));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').head(5));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').tail(5));",
            "mc_idx": 31,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Extraction",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 1.0,
                "Exploratory_Data_Analysis": 0.75,
                "Data_Transform": 0.0,
                "Model_Train": 0.0,
                "Model_Evaluation": 0.0,
                "Model_Interpretation": 0.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {
                    "read_csv": 4,
                    "pd.read_": 4
                },
                "Exploratory_Data_Analysis": {
                    ".head(": 1,
                    ".tail(": 1,
                    "head": 1,
                    "tail": 1,
                    ".head": 1,
                    ".tail": 1
                },
                "Data_Transform": {},
                "Model_Train": {},
                "Model_Evaluation": {},
                "Model_Interpretation": {},
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    18,
                    2,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": [
                        "\u001b[1m\u001b[2m\u001b[34mSample submission file\n\u001b[0m\n",
                        "   PassengerId  Survived\n0          892         0\n1          893         1\n2          894         0\n3          895         0\n4          896         1",
                        "     PassengerId  Survived\n413         1305         0\n414         1306         1\n415         1307         0\n416         1308         0\n417         1309         0"
                    ]
                },
                "mc_idx": 31,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 18,
                "o_idx": 2
            }
        },
        {
            "source": "# Adding new columns for specific mode values:-\nmdl_pred_prf['AllModels'] = mode(mdl_pred_prf, axis=1)[0];\nmdl_pred_prf['BoostedTree'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost']], axis=1)[0];\nmdl_pred_prf['Ensemble'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost', 'RandomForest']], axis=1)[0];",
            "mc_idx": 32,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Exploratory_Data_Analysis",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 1.0,
                "Data_Transform": 0.0,
                "Model_Train": 1.0,
                "Model_Evaluation": 1.0,
                "Model_Interpretation": 1.0,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 0.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {},
                "Model_Train": {
                    "model": 1
                },
                "Model_Evaluation": {
                    "model": 1
                },
                "Model_Interpretation": {
                    "model": 1
                },
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {},
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    19,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 32,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 19,
                "o_idx": 0
            }
        },
        {
            "source": "sel_col_nm = 'AllModels';\npd.DataFrame(mdl_pred_prf[sel_col_nm].values, index = xtest.PassengerId, \n             columns = ['Survived'], dtype= np.int32).reset_index().to_csv(\"Submission.csv\", index= False);",
            "mc_idx": 33,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Data_Export",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0.0,
                "Data_Extraction": 0.0,
                "Exploratory_Data_Analysis": 0.5,
                "Data_Transform": 0.5,
                "Model_Train": 0.5,
                "Model_Evaluation": 0.5,
                "Model_Interpretation": 0.5,
                "Hyperparameter_Tuning": 0.0,
                "Visualization": 0.0,
                "Debug": 0.0,
                "Data_Export": 1.0,
                "Other": 0.0
            },
            "detailed_scores": {
                "Environment": {},
                "Data_Extraction": {},
                "Exploratory_Data_Analysis": {
                    "columns": 1
                },
                "Data_Transform": {
                    ".reset_index": 1
                },
                "Model_Train": {
                    "model": 1
                },
                "Model_Evaluation": {
                    "model": 1
                },
                "Model_Interpretation": {
                    "model": 1
                },
                "Hyperparameter_Tuning": {},
                "Visualization": {},
                "Debug": {},
                "Data_Export": {
                    ".to_csv(": 1,
                    "to_csv": 1
                },
                "Other": {}
            },
            "emb": 0,
            "cell_type": "code",
            "image_path": [
                [
                    null,
                    20,
                    0,
                    null
                ]
            ],
            "output": {
                "source": {
                    "source": []
                },
                "mc_idx": 33,
                "nb_idx": 0,
                "embedding": {},
                "classification": null,
                "keywords": {},
                "summary": null,
                "q_number": null,
                "duration": null,
                "exception": null,
                "class_probability": {},
                "detailed_scores": {},
                "emb": 0,
                "cell_type": "output",
                "c_idx": 20,
                "o_idx": 0
            }
        }
    ],
    "markdown_cells": [
        {
            "source": "# Titanic- Machine Learning from disaster",
            "mc_idx": 2,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# 1. Data processing and visualization\n\nWe explore the data set, look into the features, study their distributions, assess nulls and understand the data effectively in this section.\nThese ideas will be used in the next section in the data pipeline.",
            "mc_idx": 4,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### a. Target column details:-",
            "mc_idx": 5,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### b. Passenger class and gender:-",
            "mc_idx": 7,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### c. Age:-",
            "mc_idx": 9,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### d. Ticket fare:-",
            "mc_idx": 11,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### e. Null valued columns:-",
            "mc_idx": 13,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### f. Cabin null inference in training set:-",
            "mc_idx": 15,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "## Creating pipeline adjutant functions and classes:-\n",
            "mc_idx": 17,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# 2. Model pipeline development\n\nKey pipeline steps:-\n* Treat nulls in cabin- This step treats nulls in cabin using Pclass and Fare, also fills up remaining nulls after treatment with the Pclass mode\n* Transform data- This step adds the new features and drops some redundant features.\n* Impute Embarked- This step imputes the null values in the column with the mode\n* Label encoder- This is used for the selected object columns only\n* Impute age- This is used to impute age based on the grouped median of child/ adult and gender\n* Impute fare- This fills fare column nulls based on the median of Pclass fare\n* Bin Age Fare- This bins the age and fare columns (this is not used in the latest version)\n* Robust Scaler- This is used for numerical columns only as selected in the column list",
            "mc_idx": 23,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "### Model parameter setting and implementation:-\n\nKey models and description:-\n1. Logistic Regression\n2. SVC classifier\n3. Single tree\n4. Ensemble random forest\n5. Gradient boosting machine\n6. Light GBM\n7. XgBoost Classifier\n\nModel outputs are stored in 2 tables- \n1. Model parameters profile (mdl_param_prf)- This stores the model name and relevant score metrics\n2. Model prediction profile (mdl_pred_prf) - This stores the test set predictions from each model (best estimator)",
            "mc_idx": 26,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "Implementation routine- \n1. Load the relevant model\n2. Fit the model with the grid search parameters\n3. Create relevant scores for the training set\n4. Extract test set predictions\n5. Integrate the data accordingly into the output tables",
            "mc_idx": 28,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        },
        {
            "source": "# 3. Submission file preparation\n\nWe explore a few options from the developed models and prepare the submissio file in this section. This is the last section in the model framework.",
            "mc_idx": 30,
            "nb_idx": 56,
            "embedding": {},
            "classification": "Markdown",
            "keywords": {},
            "summary": null,
            "q_number": null,
            "duration": null,
            "exception": null,
            "class_probability": {
                "Environment": 0,
                "Data_Extraction": 0,
                "Exploratory_Data_Analysis": 0,
                "Data_Transform": 0,
                "Model_Train": 0,
                "Model_Evaluation": 0,
                "Model_Interpretation": 0,
                "Hyperparameter_Tuning": 0,
                "Visualization": 0,
                "Debug": 0,
                "Data_Export": 0,
                "Other": 0
            },
            "detailed_scores": [
                {
                    "Environment": [],
                    "Data_Extraction": [],
                    "Exploratory_Data_Analysis": [],
                    "Data_Transform": [],
                    "Model_Train": [],
                    "Model_Evaluation": [],
                    "Model_Interpretation": [],
                    "Hyperparameter_Tuning": [],
                    "Visualization": [],
                    "Debug": [],
                    "Data_Export": [],
                    "Other": []
                }
            ],
            "emb": 0,
            "cell_type": "markdown"
        }
    ],
    "sim_matrix": [],
    "cell_sim_matrix": [],
    "nb_order": [],
    "summary_data_VA": null
}